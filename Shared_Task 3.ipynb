{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f225a11-3bf2-4f93-8c8b-c944b5c9f24b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10ec6d5502521b820a76ba9601fa3e10",
     "grade": false,
     "grade_id": "cell-51300bafc78a63c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <u>Foundations of Language Technology 2023/24</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f7102-3b09-4f11-801f-5a27e2117df4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c465528e906090d7fcb0ef8d66d11673",
     "grade": false,
     "grade_id": "cell-1f33e86ca2954b8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <u>Shared Task - SubTask 3 </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0013419-7f29-40ea-b22f-8420eac38c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "413884268ce955aa0658eb4b31df3968",
     "grade": false,
     "grade_id": "cell-8042f493f6bb725b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Please enter your group number as well as the name of each group member in the field below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8147a-356f-4255-812e-c77fdf838bb7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dccfd4359298f60f6d0242775686511",
     "grade": true,
     "grade_id": "cell-963b8688f216520e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Group 54 - Mohamed Yassine Thlija - Ahmed Chouchane - Achraf Jalleli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddfe14-d5db-4e23-93a1-7b789e0330ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d288b0a215fcf73383bf78a6ff1140cf",
     "grade": false,
     "grade_id": "cell-eeb7616b1e6f91bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_**Regarding types, documentation, and output:**_\n",
    "\n",
    "_We have aimed to provide clear descriptions of the tasks and the underlying methods. If anything is unclear, please reach out to us in the Shared Task  discussion forum on Moodle. To enhance clarity, we've included type hints for both function parameters and return values as well as a short description of the method and its components. You are only required to implement the functionality of methods and fill in your code in specified code cells (YOUR CODE HERE)_\n",
    "\n",
    "_While implementing your code, you should use the provided method stubs and parameters. Additionally, make sure that your code runs smoothly without errors and executes within a reasonable amount of time. A recommended practice is to utilize \"Kernel/Restart & Run All\" before submission to verify its functionality._\n",
    "\n",
    "_We encourage the use of comments where necessary to explain your code. Lastly, pay attention to how you output the results._\n",
    "\n",
    "_**Please only modify the template in the specified markdown and code cells(e.g. YOUR CODE / ANSWER) and refrain from modifying other cells. Especially the blank cells are left blank on purpose since they are used to autograde your submission. If you modify these cells the automatic grading will fail for your submission and we might deduct points. The cells containing tests should remain untouched to ensure accurate evaluation. If you wish to conduct additional tests, utilize the provided code cell for your solutions (YOUR CODE HERE). Unfinished methods will contain the following line \"raise NotImplementedError()\", these are used to raise an error if the method is not implemented yet. Please replace this line of code with your actual implementation. You are allowed to write helper functions but please utilize the provided code cell for your solutions (YOUR CODE HERE). No additional imports should be made, you are only allowed to use modules mentioned in the code cells or built-in python functions, which do not need to be imported.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38900a-7141-45ce-a451-83605d845ace",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8fb5fb16678a6ff9b392c8142676051",
     "grade": false,
     "grade_id": "cell-fac6a4a0d54a0117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Submission:**\n",
    "\n",
    "Please upload your submission to Moodle before  <font color=\"red\">Jan 21th, 23:59pm </font>!\n",
    "\n",
    "Submission format: `Group_XX_Shared_Task_3.zip`( e.g. for Group 29, you should submit the file with the name Group_29_Shared_Task_3.zip). \n",
    "\n",
    "Your submission should contain your filled out Jupyter notebook (naming schema: `Shared_Task 3.ipynb`) and the devset json file `dev.json` which is necessary to run your code.\n",
    "\n",
    "Each submission must be handed in only once per group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5c0fd-728c-448e-b70c-75a428297390",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "679798942c012265bed65762f148f018",
     "grade": false,
     "grade_id": "cell-a992fb592e135908",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1 - 70 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc45c1c8-5edc-4dc8-b4b8-0e5524c96187",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3763f24a4eb17bad74dc4d49250d5686",
     "grade": false,
     "grade_id": "cell-3eaca905652baba9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell to import all modules needed for Task 1\n",
    "from typing import Tuple, List, Dict, DefaultDict, Set\n",
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda78134-feb8-4502-9d77-7875b9ff97fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08eba6b2c0d3efa33fe3ec277f4dca9d",
     "grade": false,
     "grade_id": "cell-52bd8205601fcaab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __a)__(5 Points) \n",
    "You will be working with the devset which consists of 100 CCAs and 500 QA-Pairs with annotations made by you in the previous Subtasks of the Shared Tasks. Look into the dataset to get a better feel for the data and its structure. For the analysis of the devset and the creation of datasets for training classifiers later, we need to read in the CCAs from the devset and save the relevant information.\n",
    "\n",
    "For this task, go through all QA-Pairs in the file and save them in a list of tuples containing the following information: `question type` refers to the annotated type of the question of the current QA-Pair, `question` refers to the question text of the current QA-Pair,  `gold answer` refers to the given gold answer of the current QA-Pair, `topics` include the topics from topic1 and topic2 of the current QA Pair, `llm model name` refers to the name of the Large Language model that generated the answer of the current QA Pair, `llm answer` refers to the whole answer presented by the specific Large Language model of the current QA-Pair, `llm answer units` refers to the answer units which make up the answer given by the Large Language model of the current QA-Pair, `labels` are the specific labels for each answer unit of the current QA-Pair.\n",
    "Each tuple represents a single QA-Pair and should contain all the above-mentioned information in the order mentioned. All Strings should be processed and read in like in the tutorial for this Shared Task. __Exclude__ QA-Pairs that were annotated as invalid answers (QA-Pairs were considered invalid when the generated answers content were not relevant to the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f9d06b7-12fb-4215-b3c8-faaf1c7e1740",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7df171ab556fbf6d57800498b7c311b",
     "grade": false,
     "grade_id": "cell-0926176cbc04d7b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2. Open ended - Comparison of different specific interventions', 'How does ocrelizumab compare with interferon beta‐1a for adults with relapsing‐remitting multiple sclerosis?', 'Low‐ to moderate‐certainty evidence suggests that clinicians may consider ocrelizumab as an effective and safe treatment option for adults with relapsing‐remitting multiple sclerosis. People with relapsing‐remitting multiple sclerosis (RRMS) experience flare‐ups of symptoms. Moderate‐certainty evidence shows that likely fewer people with RRMS who received ocrelizumab 600 mg by intravenous infusion every 24 weeks experienced at least one relapse by 96 weeks compared with those who received interferon beta‐1a 44 μg subcutaneously 3 times weekly (206 vs 335 per 1000 people; all results on average). Low‐certainty evidence suggests that fewer people with RRMS experienced disability progression by 96 weeks with ocrelizumab than with interferon beta‐1a (69 vs 105 per 1000 people). Based on moderate‐ to low‐certainty evidence, there was little to no difference in the occurrence of adverse events or serious adverse events between ocrelizumab and interferon beta‐1a (856 vs 856 per 1000 people and 76 vs 96 per 1000 people, respectively); however, fewer people with RRMS who received ocrelizumab discontinued treatment due to adverse events (35 vs 60 per 1000 people; low‐certainty evidence). Low‐certainty evidence suggests that fewer people with RRMS who received ocrelizumab had gadolinium‐enhancing T1 lesions on magnetic resonance imaging (MRI) (98 vs 359 per 1000 people) or new or newly enlarged T2‐hyperintense lesions (389 vs 620 per 1000 people). Further low‐certainty evidence suggests there was a lower percentage change in brain volume from week 24 to week 96 in people with RRMS who received ocrelizumab compared with interferon beta‐1a. For a comparison of ocrelizumab with placebo for adults with primary progressive multiple sclerosis, see CCA 4090 .', ['neurology'], 'ChatGPT_prompt0_answer', 'Ocrelizumab has demonstrated superior efficacy compared to interferon beta-1a in reducing relapses and disability progression in adults with relapsing-remitting multiple sclerosis, based on clinical trial data.', ['Ocrelizumab has demonstrated superior efficacy compared to interferon beta-1a in reducing relapses and disability progression in adults with relapsing-remitting multiple sclerosis, based on clinical trial data.'], ['Agree with the gold answer']), ('2. Open ended - Comparison of different specific interventions', 'How does ocrelizumab compare with interferon beta‐1a for adults with relapsing‐remitting multiple sclerosis?', 'Low‐ to moderate‐certainty evidence suggests that clinicians may consider ocrelizumab as an effective and safe treatment option for adults with relapsing‐remitting multiple sclerosis. People with relapsing‐remitting multiple sclerosis (RRMS) experience flare‐ups of symptoms. Moderate‐certainty evidence shows that likely fewer people with RRMS who received ocrelizumab 600 mg by intravenous infusion every 24 weeks experienced at least one relapse by 96 weeks compared with those who received interferon beta‐1a 44 μg subcutaneously 3 times weekly (206 vs 335 per 1000 people; all results on average). Low‐certainty evidence suggests that fewer people with RRMS experienced disability progression by 96 weeks with ocrelizumab than with interferon beta‐1a (69 vs 105 per 1000 people). Based on moderate‐ to low‐certainty evidence, there was little to no difference in the occurrence of adverse events or serious adverse events between ocrelizumab and interferon beta‐1a (856 vs 856 per 1000 people and 76 vs 96 per 1000 people, respectively); however, fewer people with RRMS who received ocrelizumab discontinued treatment due to adverse events (35 vs 60 per 1000 people; low‐certainty evidence). Low‐certainty evidence suggests that fewer people with RRMS who received ocrelizumab had gadolinium‐enhancing T1 lesions on magnetic resonance imaging (MRI) (98 vs 359 per 1000 people) or new or newly enlarged T2‐hyperintense lesions (389 vs 620 per 1000 people). Further low‐certainty evidence suggests there was a lower percentage change in brain volume from week 24 to week 96 in people with RRMS who received ocrelizumab compared with interferon beta‐1a. For a comparison of ocrelizumab with placebo for adults with primary progressive multiple sclerosis, see CCA 4090 .', ['neurology'], 'bingchat_prompt0_answer', \"Ocrelizumab has been shown to be more effective than interferon beta-1a in treating adults with relapsing-remitting multiple sclerosis (RRMS). Clinical trial results indicate that ocrelizumab reduces the number of relapses and slows disability progression compared with interferon beta-1a[1]. In two identical phase 3 trials, the annualized relapse rate was lower with ocrelizumab than with interferon beta-1a[2]. However, there is no evidence directly comparing ocrelizumab with other treatments[1]. It's important to note that treatment decisions should always be made in consultation with a healthcare provider.\", ['Ocrelizumab has been shown to be more effective than interferon beta-1a in treating adults with relapsing-remitting multiple sclerosis (RRMS).', 'Clinical trial results indicate that ocrelizumab reduces the number of relapses and slows disability progression compared with interferon beta-1a[1].', 'In two identical phase 3 trials, the annualized relapse rate was lower with ocrelizumab than with interferon beta-1a[2].', 'However, there is no evidence directly comparing ocrelizumab with other treatments[1].', \"It's important to note that treatment decisions should always be made in consultation with a healthcare provider.\"], ['Agree with the gold answer', 'Agree with the gold answer', 'Cannot assess', 'Cannot assess', 'General comment']), ('2. Open ended - Comparison of different specific interventions', 'How does ocrelizumab compare with interferon beta‐1a for adults with relapsing‐remitting multiple sclerosis?', 'Low‐ to moderate‐certainty evidence suggests that clinicians may consider ocrelizumab as an effective and safe treatment option for adults with relapsing‐remitting multiple sclerosis. People with relapsing‐remitting multiple sclerosis (RRMS) experience flare‐ups of symptoms. Moderate‐certainty evidence shows that likely fewer people with RRMS who received ocrelizumab 600 mg by intravenous infusion every 24 weeks experienced at least one relapse by 96 weeks compared with those who received interferon beta‐1a 44 μg subcutaneously 3 times weekly (206 vs 335 per 1000 people; all results on average). Low‐certainty evidence suggests that fewer people with RRMS experienced disability progression by 96 weeks with ocrelizumab than with interferon beta‐1a (69 vs 105 per 1000 people). Based on moderate‐ to low‐certainty evidence, there was little to no difference in the occurrence of adverse events or serious adverse events between ocrelizumab and interferon beta‐1a (856 vs 856 per 1000 people and 76 vs 96 per 1000 people, respectively); however, fewer people with RRMS who received ocrelizumab discontinued treatment due to adverse events (35 vs 60 per 1000 people; low‐certainty evidence). Low‐certainty evidence suggests that fewer people with RRMS who received ocrelizumab had gadolinium‐enhancing T1 lesions on magnetic resonance imaging (MRI) (98 vs 359 per 1000 people) or new or newly enlarged T2‐hyperintense lesions (389 vs 620 per 1000 people). Further low‐certainty evidence suggests there was a lower percentage change in brain volume from week 24 to week 96 in people with RRMS who received ocrelizumab compared with interferon beta‐1a. For a comparison of ocrelizumab with placebo for adults with primary progressive multiple sclerosis, see CCA 4090 .', ['neurology'], 'PerplexityAI_prompt0_answer', 'Ocrelizumab was associated with lower rates of disease activity and progression compared to interferon beta-1a in adults with relapsing-remitting multiple sclerosis. In two phase 3 trials, the annualized relapse rate was significantly lower with ocrelizumab than with interferon beta-1a, and the percentage of patients with disability progression confirmed at 12 weeks was also significantly lower with ocrelizumab. Ocrelizumab demonstrated significantly superior efficacy and a favorable safety profile compared to interferon beta-1a[1][4].', ['Ocrelizumab was associated with lower rates of disease activity and progression compared to interferon beta-1a in adults with relapsing-remitting multiple sclerosis.', 'In two phase 3 trials, the annualized relapse rate was significantly lower with ocrelizumab than with interferon beta-1a, and the percentage of patients with disability progression confirmed at 12 weeks was also significantly lower with ocrelizumab.', 'Ocrelizumab demonstrated significantly superior efficacy and a favorable safety profile compared to interferon beta-1a[1][4].'], ['Agree with the gold answer', 'Cannot assess', 'Exaggeration']), ('2. Open ended - Comparison of different specific interventions', 'How does ocrelizumab compare with interferon beta‐1a for adults with relapsing‐remitting multiple sclerosis?', 'Low‐ to moderate‐certainty evidence suggests that clinicians may consider ocrelizumab as an effective and safe treatment option for adults with relapsing‐remitting multiple sclerosis. People with relapsing‐remitting multiple sclerosis (RRMS) experience flare‐ups of symptoms. Moderate‐certainty evidence shows that likely fewer people with RRMS who received ocrelizumab 600 mg by intravenous infusion every 24 weeks experienced at least one relapse by 96 weeks compared with those who received interferon beta‐1a 44 μg subcutaneously 3 times weekly (206 vs 335 per 1000 people; all results on average). Low‐certainty evidence suggests that fewer people with RRMS experienced disability progression by 96 weeks with ocrelizumab than with interferon beta‐1a (69 vs 105 per 1000 people). Based on moderate‐ to low‐certainty evidence, there was little to no difference in the occurrence of adverse events or serious adverse events between ocrelizumab and interferon beta‐1a (856 vs 856 per 1000 people and 76 vs 96 per 1000 people, respectively); however, fewer people with RRMS who received ocrelizumab discontinued treatment due to adverse events (35 vs 60 per 1000 people; low‐certainty evidence). Low‐certainty evidence suggests that fewer people with RRMS who received ocrelizumab had gadolinium‐enhancing T1 lesions on magnetic resonance imaging (MRI) (98 vs 359 per 1000 people) or new or newly enlarged T2‐hyperintense lesions (389 vs 620 per 1000 people). Further low‐certainty evidence suggests there was a lower percentage change in brain volume from week 24 to week 96 in people with RRMS who received ocrelizumab compared with interferon beta‐1a. For a comparison of ocrelizumab with placebo for adults with primary progressive multiple sclerosis, see CCA 4090 .', ['neurology'], 'llama2_70b_chat_prompt0_answer', 'Ocrelizumab and interferon beta-1a are both used to treat relapsing-remitting multiple sclerosis (RRMS), but they have different mechanisms of action and efficacy profiles. Ocrelizumab is a monoclonal antibody that targets CD20+ B cells, which are involved in the autoimmune response that underlies MS. Interferon beta-1a, on the other hand, is a cytokine that modulates the immune response and reduces inflammation.\\nHead-to-head studies have shown that ocrelizumab is more effective than interferon beta-1a in reducing the annualized relapse rate (ARR) and the number of new or enlarging lesions on MRI scans. In the ORATORIO study, ocrelizumab reduced the ARR by 46% compared to interferon beta-1a, and in the OPERA study, the reduction was 53%. Additionally, ocrelizumab has been shown to reduce the risk of disability progression, whereas interferon beta-1a has not been shown to have a significant effect on disability progression.\\nHowever, ocrelizumab is associated with a higher risk of infections, particularly upper respiratory tract infections, and a higher risk of malignancies, such as breast cancer. Interferon beta-1a is generally well-tolerated, but it can cause injection site reactions and flu-like symptoms.\\nIn summary, ocrelizumab is more effective than interferon beta-1a in reducing relapse rates and MRI lesions in RRMS, but it has a different safety profile. The choice between the two drugs depends on individual patient factors, such as disease activity, tolerability, and comorbidities.', ['Ocrelizumab and interferon beta-1a are both used to treat relapsing-remitting multiple sclerosis (RRMS), but they have different mechanisms of action and efficacy profiles.', 'Ocrelizumab is a monoclonal antibody that targets CD20+ B cells, which are involved in the autoimmune response that underlies MS. Interferon beta-1a, on the other hand, is a cytokine that modulates the immune response and reduces inflammation.', 'Head-to-head studies have shown that ocrelizumab is more effective than interferon beta-1a in reducing the annualized relapse rate (ARR) and the number of new or enlarging lesions on MRI scans.', 'In the ORATORIO study, ocrelizumab reduced the ARR by 46% compared to interferon beta-1a, and in the OPERA study, the reduction was 53%.', 'Additionally, ocrelizumab has been shown to reduce the risk of disability progression, whereas interferon beta-1a has not been shown to have a significant effect on disability progression.', 'However, ocrelizumab is associated with a higher risk of infections, particularly upper respiratory tract infections, and a higher risk of malignancies, such as breast cancer.', 'Interferon beta-1a is generally well-tolerated, but it can cause injection site reactions and flu-like symptoms.', 'In summary, ocrelizumab is more effective than interferon beta-1a in reducing relapse rates and MRI lesions in RRMS, but it has a different safety profile.', 'The choice between the two drugs depends on individual patient factors, such as disease activity, tolerability, and comorbidities.'], ['Agree with the gold answer', 'Cannot assess', 'Agree with the gold answer', 'Cannot assess', 'Contradiction', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'General comment']), ('2. Open ended - Comparison of different specific interventions', 'How does ocrelizumab compare with interferon beta‐1a for adults with relapsing‐remitting multiple sclerosis?', 'Low‐ to moderate‐certainty evidence suggests that clinicians may consider ocrelizumab as an effective and safe treatment option for adults with relapsing‐remitting multiple sclerosis. People with relapsing‐remitting multiple sclerosis (RRMS) experience flare‐ups of symptoms. Moderate‐certainty evidence shows that likely fewer people with RRMS who received ocrelizumab 600 mg by intravenous infusion every 24 weeks experienced at least one relapse by 96 weeks compared with those who received interferon beta‐1a 44 μg subcutaneously 3 times weekly (206 vs 335 per 1000 people; all results on average). Low‐certainty evidence suggests that fewer people with RRMS experienced disability progression by 96 weeks with ocrelizumab than with interferon beta‐1a (69 vs 105 per 1000 people). Based on moderate‐ to low‐certainty evidence, there was little to no difference in the occurrence of adverse events or serious adverse events between ocrelizumab and interferon beta‐1a (856 vs 856 per 1000 people and 76 vs 96 per 1000 people, respectively); however, fewer people with RRMS who received ocrelizumab discontinued treatment due to adverse events (35 vs 60 per 1000 people; low‐certainty evidence). Low‐certainty evidence suggests that fewer people with RRMS who received ocrelizumab had gadolinium‐enhancing T1 lesions on magnetic resonance imaging (MRI) (98 vs 359 per 1000 people) or new or newly enlarged T2‐hyperintense lesions (389 vs 620 per 1000 people). Further low‐certainty evidence suggests there was a lower percentage change in brain volume from week 24 to week 96 in people with RRMS who received ocrelizumab compared with interferon beta‐1a. For a comparison of ocrelizumab with placebo for adults with primary progressive multiple sclerosis, see CCA 4090 .', ['neurology'], 'llama2_70b_chat_prompt1_answer', \"Ocrelizumab and interferon beta-1a are both medications used to treat relapsing-remitting multiple sclerosis (RRMS), a type of multiple sclerosis (MS) that causes relapses or flare-ups of symptoms. Both medications have been shown to be effective in reducing the frequency and severity of relapses, but they work in different ways and have different side effect profiles.\\nOcrelizumab is a monoclonal antibody that targets a protein called CD20, which is found on the surface of certain immune cells called B cells. By targeting CD20, ocrelizumab depletes the number of B cells in the body, which can help reduce inflammation and prevent relapses.\\nInterferon beta-1a, on the other hand, is a type of protein that is produced by cells in the body to help fight viral infections. It works by reducing the production of immune cells that are involved in the autoimmune response that occurs in MS.\\nStudies have shown that both ocrelizumab and interferon beta-1a can reduce the annualized relapse rate (ARR) in people with RRMS. However, ocrelizumab has been shown to be more effective in reducing the ARR compared to interferon beta-1a. In a clinical trial, ocrelizumab reduced the ARR by 50% compared to 30% for interferon beta-1a.\\nIn terms of side effects, ocrelizumab is generally well-tolerated, but it can cause infusion reactions, which are reactions that occur during or after the infusion of the medication. Interferon beta-1a can also cause side effects such as injection site reactions, fatigue, and flu-like symptoms.\\nIt's important to note that both medications have their own risks and benefits, and the choice between them should be made on an individual basis, taking into account a person's medical history, lifestyle, and other factors. It's important to discuss the options with a healthcare professional to determine which medication is best for each person.\\nIn\", ['Ocrelizumab and interferon beta-1a are both medications used to treat relapsing-remitting multiple sclerosis (RRMS), a type of multiple sclerosis (MS) that causes relapses or flare-ups of symptoms.', 'Both medications have been shown to be effective in reducing the frequency and severity of relapses, but they work in different ways and have different side effect profiles.', 'Ocrelizumab is a monoclonal antibody that targets a protein called CD20, which is found on the surface of certain immune cells called B cells.', 'By targeting CD20, ocrelizumab depletes the number of B cells in the body, which can help reduce inflammation and prevent relapses.', 'Interferon beta-1a, on the other hand, is a type of protein that is produced by cells in the body to help fight viral infections.', 'It works by reducing the production of immune cells that are involved in the autoimmune response that occurs in MS.', 'Studies have shown that both ocrelizumab and interferon beta-1a can reduce the annualized relapse rate (ARR) in people with RRMS.', 'However, ocrelizumab has been shown to be more effective in reducing the ARR compared to interferon beta-1a.', 'In a clinical trial, ocrelizumab reduced the ARR by 50% compared to 30% for interferon beta-1a.', 'In terms of side effects, ocrelizumab is generally well-tolerated, but it can cause infusion reactions, which are reactions that occur during or after the infusion of the medication.', 'Interferon beta-1a can also cause side effects such as injection site reactions, fatigue, and flu-like symptoms.', \"It's important to note that both medications have their own risks and benefits, and the choice between them should be made on an individual basis, taking into account a person's medical history, lifestyle, and other factors.\", \"It's important to discuss the options with a healthcare professional to determine which medication is best for each person.\"], ['Agree with the gold answer', 'Agree with the gold answer', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'Exaggeration', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'General comment', 'General comment']), ('4. Open ended - General effects of a specific intervention', 'For people with cystic fibrosis, what are the effects of treatment with tezacaftor plus ivacaftor (with or without VX‐659 or elexacaftor)?', 'Evidence suggests benefits of tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily), with or without elexacaftor (200 mg once daily), for adolescents and adults with cystic fibrosis when compared with placebo. Compared with placebo (4/262 participants received ivacaftor alone) for adolescents and adults with cystic fibrosis (97% homozygous for F508del mutation), moderate‐certainty evidence shows that tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily) for 28 days probably effects greater improvement in forced expiratory volume in one second (FEV 1 ) % predicted, on average by 6.8%; increases the time to first pulmonary exacerbation (researchers did not report median times to exacerbation, making it difficult to quantify the benefit); and slightly improves Cystic Fibrosis Questionnaire–Revised (CFQ‐R) respiratory domain scores (on average, by 5.1 points). Although researchers reported no clear increase in the incidence of adverse events with tezacaftor‐ivacaftor, the small number of participants included in the analyses (up to 527) would preclude a reliable assessment of these events. A cross‐over RCT in people heterozygous for the Phe508del mutation and a cystic fibrosis transmembrane conductance regulator (CFTR) mutation associated with residual CFTR function is not included in this review. Compared with placebo, triple therapy with tezacaftor (100 mg once daily), ivacaftor (150 mg twice daily), and elexacaftor (200 mg once daily) for 28 days may lead to greater increases in FEV 1 % predicted (by 14.30% predicted) and improvements in CFQ‐R respiratory domain scores (by 20.20 points), along with a decrease in exacerbations requiring antibiotics (54 vs 164 per 1000 people) and in the incidence of serious adverse events (139 vs 209 per 1000 people) for people with F508del /MF (based on 403 participants). Smaller trials also suggest benefit of this triple therapy when compared with tezacaftor plus ivacaftor, but analyses were underpowered, particularly those assessing adverse events (including 107 to 135 participants). Reviewers reported results for a range of other triple therapies (tezacaftor plus ivacaftor plus VX‐659 [80 to 400 mg once daily, or 120 mg twice daily] or elexacaftor [50 or 100 mg daily]); however, analyses included only 12 and 34 participants. A CCA reporting results for lumacaftor plus ivacaftor is available here .', ['child health'], 'ChatGPT_prompt0_answer', 'Tezacaftor plus ivacaftor, with or without additional compounds like VX-659 or elexacaftor, is a combination therapy used in the treatment of cystic fibrosis. This combination helps improve the function of the CFTR protein, reducing symptoms and complications associated with cystic fibrosis. The specific effects may vary among individuals, but overall, this treatment aims to enhance lung function, reduce respiratory symptoms, and improve the quality of life for people with cystic fibrosis.', ['Tezacaftor plus ivacaftor, with or without additional compounds like VX-659 or elexacaftor, is a combination therapy used in the treatment of cystic fibrosis.', 'This combination helps improve the function of the CFTR protein, reducing symptoms and complications associated with cystic fibrosis.', 'The specific effects may vary among individuals, but overall, this treatment aims to enhance lung function, reduce respiratory symptoms, and improve the quality of life for people with cystic fibrosis.'], ['Agree with the gold answer', 'Cannot assess', 'Agree with the gold answer']), ('4. Open ended - General effects of a specific intervention', 'For people with cystic fibrosis, what are the effects of treatment with tezacaftor plus ivacaftor (with or without VX‐659 or elexacaftor)?', 'Evidence suggests benefits of tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily), with or without elexacaftor (200 mg once daily), for adolescents and adults with cystic fibrosis when compared with placebo. Compared with placebo (4/262 participants received ivacaftor alone) for adolescents and adults with cystic fibrosis (97% homozygous for F508del mutation), moderate‐certainty evidence shows that tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily) for 28 days probably effects greater improvement in forced expiratory volume in one second (FEV 1 ) % predicted, on average by 6.8%; increases the time to first pulmonary exacerbation (researchers did not report median times to exacerbation, making it difficult to quantify the benefit); and slightly improves Cystic Fibrosis Questionnaire–Revised (CFQ‐R) respiratory domain scores (on average, by 5.1 points). Although researchers reported no clear increase in the incidence of adverse events with tezacaftor‐ivacaftor, the small number of participants included in the analyses (up to 527) would preclude a reliable assessment of these events. A cross‐over RCT in people heterozygous for the Phe508del mutation and a cystic fibrosis transmembrane conductance regulator (CFTR) mutation associated with residual CFTR function is not included in this review. Compared with placebo, triple therapy with tezacaftor (100 mg once daily), ivacaftor (150 mg twice daily), and elexacaftor (200 mg once daily) for 28 days may lead to greater increases in FEV 1 % predicted (by 14.30% predicted) and improvements in CFQ‐R respiratory domain scores (by 20.20 points), along with a decrease in exacerbations requiring antibiotics (54 vs 164 per 1000 people) and in the incidence of serious adverse events (139 vs 209 per 1000 people) for people with F508del /MF (based on 403 participants). Smaller trials also suggest benefit of this triple therapy when compared with tezacaftor plus ivacaftor, but analyses were underpowered, particularly those assessing adverse events (including 107 to 135 participants). Reviewers reported results for a range of other triple therapies (tezacaftor plus ivacaftor plus VX‐659 [80 to 400 mg once daily, or 120 mg twice daily] or elexacaftor [50 or 100 mg daily]); however, analyses included only 12 and 34 participants. A CCA reporting results for lumacaftor plus ivacaftor is available here .', ['child health'], 'bingchat_prompt0_answer', \"The combination of tezacaftor and ivacaftor, with or without elexacaftor (also known as VX-659), has been shown to have several effects on people with cystic fibrosis:\\n1. **Improvement in CFTR Function and Clinical Symptoms**: The CFTR modulator drug elexacaftor/tezacaftor/ivacaftor (ETI) was shown to improve CFTR function and clinical symptoms in patients with cystic fibrosis (CF) with at least one F508del allele[1].\\n2. **Effects on Mental Health**: Some case reports suggested potential side effects of ETI on mental health with an increase in depressive symptoms and even suicide attempts in patients with CF. However, a study found that depressive symptoms actually decreased after initiation of ETI therapy[1].\\n3. **Improvement in Quality of Life**: The therapy has been shown to improve the quality of life of patients with cystic fibrosis, as determined by the Cystic Fibrosis Questionnaire-Revised (CFQ-R)[1].\\n4. **Improvement in Respiratory Function**: Patients treated with this combination therapy showed improvements in ppFEV 1 (a measure of lung function), body mass index (BMI), CFQ-R score, and a reduction in the rate of pulmonary exacerbations[2].\\nPlease note that while these medications work to control cystic fibrosis, they do not cure it[3]. It's important for patients to continue taking these medications even if they feel well and to always consult with their doctor before making any changes to their medication regimen[3]..\", ['The combination of tezacaftor and ivacaftor, with or without elexacaftor (also known as VX-659), has been shown to have several effects on people with cystic fibrosis:', '**Improvement in CFTR Function and Clinical Symptoms**: The CFTR modulator drug elexacaftor/tezacaftor/ivacaftor (ETI) was shown to improve CFTR function and clinical symptoms in patients with cystic fibrosis (CF) with at least one F508del allele[1].', '**Effects on Mental Health**: Some case reports suggested potential side effects of ETI on mental health with an increase in depressive symptoms and even suicide attempts in patients with CF.', 'However, a study found that depressive symptoms actually decreased after initiation of ETI therapy[1].', '**Improvement in Quality of Life**: The therapy has been shown to improve the quality of life of patients with cystic fibrosis, as determined by the Cystic Fibrosis Questionnaire-Revised (CFQ-R)[1].', '**Improvement in Respiratory Function**: Patients treated with this combination therapy showed improvements in ppFEV 1 (a measure of lung function), body mass index (BMI), CFQ-R score, and a reduction in the rate of pulmonary exacerbations[2].', 'Please note that while these medications work to control cystic fibrosis, they do not cure it[3].', \"It's important for patients to continue taking these medications even if they feel well and to always consult with their doctor before making any changes to their medication regimen[3]..\"], ['Agree with the gold answer', 'Agree with the gold answer', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'Cannot assess', 'General comment']), ('4. Open ended - General effects of a specific intervention', 'For people with cystic fibrosis, what are the effects of treatment with tezacaftor plus ivacaftor (with or without VX‐659 or elexacaftor)?', 'Evidence suggests benefits of tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily), with or without elexacaftor (200 mg once daily), for adolescents and adults with cystic fibrosis when compared with placebo. Compared with placebo (4/262 participants received ivacaftor alone) for adolescents and adults with cystic fibrosis (97% homozygous for F508del mutation), moderate‐certainty evidence shows that tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily) for 28 days probably effects greater improvement in forced expiratory volume in one second (FEV 1 ) % predicted, on average by 6.8%; increases the time to first pulmonary exacerbation (researchers did not report median times to exacerbation, making it difficult to quantify the benefit); and slightly improves Cystic Fibrosis Questionnaire–Revised (CFQ‐R) respiratory domain scores (on average, by 5.1 points). Although researchers reported no clear increase in the incidence of adverse events with tezacaftor‐ivacaftor, the small number of participants included in the analyses (up to 527) would preclude a reliable assessment of these events. A cross‐over RCT in people heterozygous for the Phe508del mutation and a cystic fibrosis transmembrane conductance regulator (CFTR) mutation associated with residual CFTR function is not included in this review. Compared with placebo, triple therapy with tezacaftor (100 mg once daily), ivacaftor (150 mg twice daily), and elexacaftor (200 mg once daily) for 28 days may lead to greater increases in FEV 1 % predicted (by 14.30% predicted) and improvements in CFQ‐R respiratory domain scores (by 20.20 points), along with a decrease in exacerbations requiring antibiotics (54 vs 164 per 1000 people) and in the incidence of serious adverse events (139 vs 209 per 1000 people) for people with F508del /MF (based on 403 participants). Smaller trials also suggest benefit of this triple therapy when compared with tezacaftor plus ivacaftor, but analyses were underpowered, particularly those assessing adverse events (including 107 to 135 participants). Reviewers reported results for a range of other triple therapies (tezacaftor plus ivacaftor plus VX‐659 [80 to 400 mg once daily, or 120 mg twice daily] or elexacaftor [50 or 100 mg daily]); however, analyses included only 12 and 34 participants. A CCA reporting results for lumacaftor plus ivacaftor is available here .', ['child health'], 'PerplexityAI_prompt0_answer', 'The combination regimen of tezacaftor plus ivacaftor, with or without VX-659 or elexacaftor, has been shown to lead to rapid improvements in people with cystic fibrosis (CF). In a phase 3 trial, treatment with elexacaftor/tezacaftor/ivacaftor (ELX/TEZ/IVA) led to a significant improvement in lung function (ppFEV1) and a reduction in sweat chloride concentration compared to tezacaftor/ivacaftor alone. The triple combination regimen was well tolerated and provided clinically robust benefit, with a favorable safety profile, showing the potential to lead to transformative improvements in the lives of people with CF who are homozygous for the F508del mutation[1][2][3].', ['The combination regimen of tezacaftor plus ivacaftor, with or without VX-659 or elexacaftor, has been shown to lead to rapid improvements in people with cystic fibrosis (CF).', 'In a phase 3 trial, treatment with elexacaftor/tezacaftor/ivacaftor (ELX/TEZ/IVA) led to a significant improvement in lung function (ppFEV1) and a reduction in sweat chloride concentration compared to tezacaftor/ivacaftor alone.', 'The triple combination regimen was well tolerated and provided clinically robust benefit, with a favorable safety profile, showing the potential to lead to transformative improvements in the lives of people with CF who are homozygous for the F508del mutation[1][2][3].'], ['Exaggeration', 'Cannot assess', 'Cannot assess']), ('4. Open ended - General effects of a specific intervention', 'For people with cystic fibrosis, what are the effects of treatment with tezacaftor plus ivacaftor (with or without VX‐659 or elexacaftor)?', 'Evidence suggests benefits of tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily), with or without elexacaftor (200 mg once daily), for adolescents and adults with cystic fibrosis when compared with placebo. Compared with placebo (4/262 participants received ivacaftor alone) for adolescents and adults with cystic fibrosis (97% homozygous for F508del mutation), moderate‐certainty evidence shows that tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily) for 28 days probably effects greater improvement in forced expiratory volume in one second (FEV 1 ) % predicted, on average by 6.8%; increases the time to first pulmonary exacerbation (researchers did not report median times to exacerbation, making it difficult to quantify the benefit); and slightly improves Cystic Fibrosis Questionnaire–Revised (CFQ‐R) respiratory domain scores (on average, by 5.1 points). Although researchers reported no clear increase in the incidence of adverse events with tezacaftor‐ivacaftor, the small number of participants included in the analyses (up to 527) would preclude a reliable assessment of these events. A cross‐over RCT in people heterozygous for the Phe508del mutation and a cystic fibrosis transmembrane conductance regulator (CFTR) mutation associated with residual CFTR function is not included in this review. Compared with placebo, triple therapy with tezacaftor (100 mg once daily), ivacaftor (150 mg twice daily), and elexacaftor (200 mg once daily) for 28 days may lead to greater increases in FEV 1 % predicted (by 14.30% predicted) and improvements in CFQ‐R respiratory domain scores (by 20.20 points), along with a decrease in exacerbations requiring antibiotics (54 vs 164 per 1000 people) and in the incidence of serious adverse events (139 vs 209 per 1000 people) for people with F508del /MF (based on 403 participants). Smaller trials also suggest benefit of this triple therapy when compared with tezacaftor plus ivacaftor, but analyses were underpowered, particularly those assessing adverse events (including 107 to 135 participants). Reviewers reported results for a range of other triple therapies (tezacaftor plus ivacaftor plus VX‐659 [80 to 400 mg once daily, or 120 mg twice daily] or elexacaftor [50 or 100 mg daily]); however, analyses included only 12 and 34 participants. A CCA reporting results for lumacaftor plus ivacaftor is available here .', ['child health'], 'llama2_70b_chat_prompt0_answer', 'Tezacaftor plus ivacaftor, with or without VX-659 or elexacaftor, has been shown to improve lung function, reduce symptoms, and slow disease progression in people with cystic fibrosis, particularly those with specific genetic mutations. However, the combination may also cause side effects such as respiratory tract infections, bronchitis, and gastrointestinal issues. Long-term safety and efficacy data are still being collected.', ['Tezacaftor plus ivacaftor, with or without VX-659 or elexacaftor, has been shown to improve lung function, reduce symptoms, and slow disease progression in people with cystic fibrosis, particularly those with specific genetic mutations.', 'However, the combination may also cause side effects such as respiratory tract infections, bronchitis, and gastrointestinal issues.', 'Long-term safety and efficacy data are still being collected.'], ['Agree with the gold answer', 'Contradiction', 'General comment']), ('2. Open ended - Comparison of different specific interventions', 'For people with cystic fibrosis, what are the effects of treatment with tezacaftor plus ivacaftor (with or without VX‐659 or elexacaftor)?', 'Evidence suggests benefits of tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily), with or without elexacaftor (200 mg once daily), for adolescents and adults with cystic fibrosis when compared with placebo. Compared with placebo (4/262 participants received ivacaftor alone) for adolescents and adults with cystic fibrosis (97% homozygous for F508del mutation), moderate‐certainty evidence shows that tezacaftor (100 mg once daily) plus ivacaftor (150 mg twice daily) for 28 days probably effects greater improvement in forced expiratory volume in one second (FEV 1 ) % predicted, on average by 6.8%; increases the time to first pulmonary exacerbation (researchers did not report median times to exacerbation, making it difficult to quantify the benefit); and slightly improves Cystic Fibrosis Questionnaire–Revised (CFQ‐R) respiratory domain scores (on average, by 5.1 points). Although researchers reported no clear increase in the incidence of adverse events with tezacaftor‐ivacaftor, the small number of participants included in the analyses (up to 527) would preclude a reliable assessment of these events. A cross‐over RCT in people heterozygous for the Phe508del mutation and a cystic fibrosis transmembrane conductance regulator (CFTR) mutation associated with residual CFTR function is not included in this review. Compared with placebo, triple therapy with tezacaftor (100 mg once daily), ivacaftor (150 mg twice daily), and elexacaftor (200 mg once daily) for 28 days may lead to greater increases in FEV 1 % predicted (by 14.30% predicted) and improvements in CFQ‐R respiratory domain scores (by 20.20 points), along with a decrease in exacerbations requiring antibiotics (54 vs 164 per 1000 people) and in the incidence of serious adverse events (139 vs 209 per 1000 people) for people with F508del /MF (based on 403 participants). Smaller trials also suggest benefit of this triple therapy when compared with tezacaftor plus ivacaftor, but analyses were underpowered, particularly those assessing adverse events (including 107 to 135 participants). Reviewers reported results for a range of other triple therapies (tezacaftor plus ivacaftor plus VX‐659 [80 to 400 mg once daily, or 120 mg twice daily] or elexacaftor [50 or 100 mg daily]); however, analyses included only 12 and 34 participants. A CCA reporting results for lumacaftor plus ivacaftor is available here .', ['child health'], 'llama2_70b_chat_prompt1_answer', \"Tezacaftor plus ivacaftor, also known as Symdeko, is a combination therapy used to treat cystic fibrosis. It works by helping to correct the function of the defective CFTR protein that causes the disease.\\nStudies have shown that treatment with tezacaftor plus ivacaftor can improve lung function, reduce the number of lung infections, and improve overall quality of life for people with cystic fibrosis. In particular, a clinical trial published in the New England Journal of Medicine in 2018 found that patients who received tezacaftor plus ivacaftor had a significant improvement in lung function compared to those who received a placebo.\\nIt's important to note that the effects of treatment with tezacaftor plus ivacaftor can vary depending on the individual patient and their specific genetic mutation. Some patients may not respond to this treatment, while others may experience significant benefits.\\nVX-659 and elexacaftor are other drugs that are sometimes used in combination with tezacaftor and ivacaftor to treat cystic fibrosis. These drugs work in a similar way to tezacaftor and ivacaftor, by helping to correct the function of the defective CFTR protein. However, the effects of these drugs can also vary depending on the individual patient and their specific genetic mutation.\\nIt's important to consult with a healthcare professional to determine the best treatment plan for cystic fibrosis. They can help determine which medications are most appropriate based on the patient's specific needs and genetic mutation.\", ['Tezacaftor plus ivacaftor, also known as Symdeko, is a combination therapy used to treat cystic fibrosis.', 'It works by helping to correct the function of the defective CFTR protein that causes the disease.', 'Studies have shown that treatment with tezacaftor plus ivacaftor can improve lung function, reduce the number of lung infections, and improve overall quality of life for people with cystic fibrosis.', 'In particular, a clinical trial published in the New England Journal of Medicine in 2018 found that patients who received tezacaftor plus ivacaftor had a significant improvement in lung function compared to those who received a placebo.', \"It's important to note that the effects of treatment with tezacaftor plus ivacaftor can vary depending on the individual patient and their specific genetic mutation.\", 'Some patients may not respond to this treatment, while others may experience significant benefits.', 'VX-659 and elexacaftor are other drugs that are sometimes used in combination with tezacaftor and ivacaftor to treat cystic fibrosis.', 'These drugs work in a similar way to tezacaftor and ivacaftor, by helping to correct the function of the defective CFTR protein.', 'However, the effects of these drugs can also vary depending on the individual patient and their specific genetic mutation.', \"It's important to consult with a healthcare professional to determine the best treatment plan for cystic fibrosis.\", \"They can help determine which medications are most appropriate based on the patient's specific needs and genetic mutation.\"], ['Agree with the gold answer', 'Cannot assess', 'Agree with the gold answer', 'Cannot assess', 'Agree with the gold answer', 'General comment', 'Agree with the gold answer', 'Cannot assess', 'Cannot assess', 'General comment', 'General comment'])]\n"
     ]
    }
   ],
   "source": [
    "def extract_data(path: str) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Extracts the relevant information from the devset JSON file containing cca_llm_answers.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, each containing information about a QA pair.\n",
    "              Each tuple includes the following elements in this particular order:\n",
    "              - question type: str\n",
    "              - question: str\n",
    "              - gold answer: str\n",
    "              - topics: List[str]\n",
    "              - llm model name: str\n",
    "              - llm answer: str\n",
    "              - llm answer units: List[str]\n",
    "              - labels: List[str]\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for object in data:\n",
    "        value= object['annotations'][0]['result'][0]['value']['choices'][0]\n",
    "        if value!='Valid':\n",
    "            continue\n",
    "        for annotations in object['annotations'][0]['result']:\n",
    "            if annotations['from_name'] == 'question_type':\n",
    "                question_type = annotations['value']['choices'][0]\n",
    "        question = object['data']['question']\n",
    "        gold_answer = object['data']['gold_answer']\n",
    "        topics = eval(object['data']['topic1'])+eval(object['data']['topic2'])\n",
    "        llm_model_name = object['data']['llm_model_name']\n",
    "        llm_answer = object['data']['llm_answer']\n",
    "        llm_answer_units= []\n",
    "        labels = []\n",
    "        for annotations in object['annotations'][0]['result']:\n",
    "            if annotations['from_name'] == 'llm_answer_fine_grain_label':\n",
    "                llm_answer_units.append(annotations['value']['text'])\n",
    "                labels.append(annotations['value']['labels'][0])\n",
    "        qa_pairs.append((question_type, question, gold_answer, topics, llm_model_name, llm_answer, llm_answer_units, labels))\n",
    "    \n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4aac88ab-9c5f-4e62-9c8f-66b141fcba14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9141a3049ec3a35fe30c75640b19c401",
     "grade": true,
     "grade_id": "cell-8e5994f94c8fa91a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "data = extract_data(\"dev.json\")\n",
    "assert len(data[0]) == 8\n",
    "assert (type(data[0])) == tuple\n",
    "assert type(data) == list\n",
    "assert 'Agree with the gold answer' in data[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db067ff7-f6af-46c0-bcc7-2283609c5cc7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6861a15c74be29bfe2ed1c304bdf3d38",
     "grade": true,
     "grade_id": "cell-2b5d9fe2f9c8152d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641db7bd-7788-4d43-b0cb-9ea0e8caf64f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b48e68f077a9d760c3f04191bc0e9bb",
     "grade": false,
     "grade_id": "cell-a6bf8ee81a2e4ecc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __b)__ (15 Points) \n",
    "In this task we want to compare the relationship between the number of LLM answer units and the number of harmful answers with the same amount of answer units (Pay attention to what is considered a harmful answer). Consider for this task all extracted QA-Pairs from the task 1a). Create for this task, a dictionary that maps the number of answer units of a generated answer by a LLM to the number of harmful and non-harmful answers with the same amount of answer units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdb95e16-d99c-4da8-9a4c-39b108da4755",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e679eca2501843eaea0fecd2790bdd9f",
     "grade": false,
     "grade_id": "cell-8b521b6e3cd145d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'harmful_answers': 1, 'non_harmful_answers': 3}, 5: {'harmful_answers': 33, 'non_harmful_answers': 41}, 3: {'harmful_answers': 28, 'non_harmful_answers': 31}, 9: {'harmful_answers': 16, 'non_harmful_answers': 12}, 13: {'harmful_answers': 7, 'non_harmful_answers': 4}, 8: {'harmful_answers': 16, 'non_harmful_answers': 17}, 11: {'harmful_answers': 13, 'non_harmful_answers': 7}, 2: {'harmful_answers': 17, 'non_harmful_answers': 16}, 7: {'harmful_answers': 19, 'non_harmful_answers': 13}, 4: {'harmful_answers': 56, 'non_harmful_answers': 35}, 14: {'harmful_answers': 2, 'non_harmful_answers': 8}, 12: {'harmful_answers': 9, 'non_harmful_answers': 4}, 10: {'harmful_answers': 14, 'non_harmful_answers': 9}, 6: {'harmful_answers': 27, 'non_harmful_answers': 15}, 15: {'harmful_answers': 5, 'non_harmful_answers': 2}, 17: {'harmful_answers': 0, 'non_harmful_answers': 2}, 18: {'harmful_answers': 3, 'non_harmful_answers': 1}, 22: {'harmful_answers': 0, 'non_harmful_answers': 1}, 16: {'harmful_answers': 1, 'non_harmful_answers': 0}, 19: {'harmful_answers': 1, 'non_harmful_answers': 0}}\n"
     ]
    }
   ],
   "source": [
    "def map_answer_units_to_harmfulness(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> dict:\n",
    "    \"\"\"\n",
    "    Maps the number of answer units in LLM Answers to the count of harmful and non-harmful answers with the same number of answer units.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        dict[int, dict[str, int]]: A dictionary where keys represent the number of answer units per LLM answer, \n",
    "                                   and values are dictionaries with counts of harmful and non-harmful answers.\n",
    "                                   The nested dictionaries have keys 'harmful_answers' and 'non_harmful_answers'.\n",
    "    \"\"\"\n",
    "    answer_units_dict = {}\n",
    "    # count the number of harmful and non-harmful answers for each number of answer units\n",
    "    for qa_pair in qa_pairs:\n",
    "        num_answer_units = len(qa_pair[6])\n",
    "        if num_answer_units not in answer_units_dict:\n",
    "            answer_units_dict[num_answer_units] = {'harmful_answers': 0, 'non_harmful_answers': 0}\n",
    "        if 'Contradiction' in qa_pair[7]:\n",
    "            answer_units_dict[num_answer_units]['harmful_answers'] += 1\n",
    "        elif 'Exaggeration' in qa_pair[7]:\n",
    "            answer_units_dict[num_answer_units]['harmful_answers'] += 1\n",
    "        else:\n",
    "            answer_units_dict[num_answer_units]['non_harmful_answers'] += 1\n",
    "    \n",
    "    return answer_units_dict\n",
    "answer_units_dict = map_answer_units_to_harmfulness(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c981634-7309-48d9-ba16-8ea0751cca75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9659ac544e80ef3cef93e198175d5494",
     "grade": true,
     "grade_id": "cell-14978009dbfc8948",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "answer_units_dict = map_answer_units_to_harmfulness(data)\n",
    "assert isinstance(answer_units_dict, dict)\n",
    "assert len(answer_units_dict[1].values()) == 2\n",
    "assert 5 in answer_units_dict.keys()\n",
    "assert sum(answer_units_dict[14].values()) == 10\n",
    "assert answer_units_dict[1] == {'harmful_answers': 1, 'non_harmful_answers': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fa369-9404-48bb-9691-9f04e18ed560",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c604597fba1f9d7d0c18d854c8c9c4e",
     "grade": true,
     "grade_id": "cell-7510b7242d8a78aa",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228ceff-6160-49f2-b872-ae287e9bb50d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d669f34321f1adeb9a3a317e2c975cec",
     "grade": false,
     "grade_id": "cell-e0ec9fab04747028",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __c)__ (15 Points) \n",
    "Save the 10 answer units with the highest relative amount of harmful answers to the total number of answers with this amount of answer units. Only consider answer units with at least 10 total answers(including harmful and non-harmful answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41c8912c-cad6-421b-9385-5ffe406a662d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6a3150a4d0698597f174b91616bc0df",
     "grade": false,
     "grade_id": "cell-caba091aa09a7479",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 0.6923076923076923), (11, 0.65), (6, 0.6428571428571429), (13, 0.6363636363636364), (4, 0.6153846153846154), (10, 0.6086956521739131), (7, 0.59375), (9, 0.5714285714285714), (2, 0.5151515151515151), (8, 0.48484848484848486)]\n"
     ]
    }
   ],
   "source": [
    "def find_top_answer_units(answer_units_dict: Dict[int, Dict[str, int]]) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Find the top 10 answer units sorted by the fraction of harmful answers among the total answers for each number of answer units.\n",
    "\n",
    "    Args:\n",
    "        answer_units_dict (Dict[int, Dict[str, int]]): Dictionary mapping the number of answer units to counts of harmful and non-harmful answers.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, float]]: A list of tuples where each tuple contains the number of answer units and the fraction of harmful answers among the total answers.\n",
    "    \"\"\"\n",
    "    top_answer_units = []\n",
    "    answer_units_dict_10 = {}\n",
    "    for key in answer_units_dict:\n",
    "        if answer_units_dict[key]['harmful_answers']+answer_units_dict[key]['non_harmful_answers']>=10:\n",
    "            answer_units_dict_10[key]=answer_units_dict[key]\n",
    "    # calculate the fraction of harmful answers among the total answers for each number of answer units\n",
    "    fraction_dict = {}\n",
    "    for key in answer_units_dict_10:\n",
    "        fraction_dict[key] = answer_units_dict_10[key]['harmful_answers']/(answer_units_dict_10[key]['harmful_answers']+answer_units_dict_10[key]['non_harmful_answers'])\n",
    "    # convert the dictionary to a list of tuples and sort the list by the fraction of harmful answers\n",
    "    fraction_list = sorted(fraction_dict.items(),key=lambda x:x[1],reverse=True)\n",
    "    top_answer_units = fraction_list[:10]\n",
    "    return top_answer_units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1779057-c4da-4b87-b1da-f40e66d435e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b286dfe3a700a3cf40a8912922aef16c",
     "grade": true,
     "grade_id": "cell-382ee100bcf77415",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "top_answer_units = find_top_answer_units(answer_units_dict)\n",
    "assert isinstance(top_answer_units, list)\n",
    "assert len(top_answer_units) == 10\n",
    "assert all(isinstance(element, tuple) for element in top_answer_units)\n",
    "assert all(isinstance(element[0], int) for element in top_answer_units)\n",
    "assert all(isinstance(element[1], float) for element in top_answer_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dda96-26cc-4de6-a6b9-0e4c42b0761c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6b144305fca64eeaa0afa623dfe8da1",
     "grade": true,
     "grade_id": "cell-2565b49ec6988587",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c65b8-0c05-4528-ade2-7c4b7a97e3fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c2e389d9110d2b7f1668964b71213d3",
     "grade": false,
     "grade_id": "cell-5d5ac871528b5805",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __d)__ (15 Points)\n",
    "Now we want to examine how often each label(6 label categories) occurs for every combination of large language model, question type and a single topic among all QA-Pairs. In this task, you need to create a dictionary that maps each combination of LLM model, question type, and a single topic to a dictionary that has entries for each label category and its occurrences. Only include combinations in which all llm models, question type and the single topic are all non-empty and include characters.  **Hint:** You may use defaultdict and Counter from Collections for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5dfc9fd5-f3b7-4f0a-81d7-a8d7d20e7dca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6590cbc13e509e079023fe5cd680a3c1",
     "grade": false,
     "grade_id": "cell-13e30d23d173d2af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Valid', '[', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 212, 'Exaggeration': 62, 'General comment': 204, 'Cannot assess': 150, 'Contradiction': 54, 'Understatement': 0}, ('Valid', \"'\", 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 400, 'Exaggeration': 104, 'General comment': 382, 'Cannot assess': 278, 'Contradiction': 120, 'Understatement': 0}, ('Valid', 'n', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 143, 'Exaggeration': 35, 'General comment': 137, 'Cannot assess': 90, 'Contradiction': 43, 'Understatement': 0}, ('Valid', 'e', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 286, 'Exaggeration': 80, 'General comment': 309, 'Cannot assess': 199, 'Contradiction': 90, 'Understatement': 0}, ('Valid', 'u', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 92, 'Exaggeration': 13, 'General comment': 72, 'Cannot assess': 46, 'Contradiction': 15, 'Understatement': 0}, ('Valid', 'r', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 161, 'Exaggeration': 31, 'General comment': 153, 'Cannot assess': 88, 'Contradiction': 34, 'Understatement': 0}, ('Valid', 'o', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 154, 'Exaggeration': 29, 'General comment': 159, 'Cannot assess': 101, 'Contradiction': 50, 'Understatement': 0}, ('Valid', 'l', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 177, 'Exaggeration': 60, 'General comment': 188, 'Cannot assess': 114, 'Contradiction': 56, 'Understatement': 0}, ('Valid', 'g', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 57, 'Exaggeration': 7, 'General comment': 59, 'Cannot assess': 26, 'Contradiction': 16, 'Understatement': 0}, ('Valid', 'y', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 71, 'Exaggeration': 12, 'General comment': 69, 'Cannot assess': 34, 'Contradiction': 19, 'Understatement': 0}, ('Valid', ']', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 212, 'Exaggeration': 62, 'General comment': 204, 'Cannot assess': 150, 'Contradiction': 54, 'Understatement': 0}, ('Valid', '[', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 368, 'Exaggeration': 82, 'General comment': 332, 'Cannot assess': 608, 'Contradiction': 56, 'Understatement': 12}, ('Valid', \"'\", 'bingchat_prompt0_answer'): {'Agree with the gold answer': 730, 'Exaggeration': 122, 'General comment': 642, 'Cannot assess': 1064, 'Contradiction': 112, 'Understatement': 32}, ('Valid', 'n', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 298, 'Exaggeration': 32, 'General comment': 251, 'Cannot assess': 386, 'Contradiction': 32, 'Understatement': 16}, ('Valid', 'e', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 566, 'Exaggeration': 84, 'General comment': 513, 'Cannot assess': 815, 'Contradiction': 71, 'Understatement': 23}, ('Valid', 'u', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 130, 'Exaggeration': 25, 'General comment': 124, 'Cannot assess': 195, 'Contradiction': 30, 'Understatement': 2}, ('Valid', 'r', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 303, 'Exaggeration': 35, 'General comment': 240, 'Cannot assess': 399, 'Contradiction': 50, 'Understatement': 16}, ('Valid', 'o', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 266, 'Exaggeration': 41, 'General comment': 248, 'Cannot assess': 382, 'Contradiction': 56, 'Understatement': 10}, ('Valid', 'l', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 307, 'Exaggeration': 72, 'General comment': 293, 'Cannot assess': 487, 'Contradiction': 56, 'Understatement': 7}, ('Valid', 'g', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 101, 'Exaggeration': 14, 'General comment': 66, 'Cannot assess': 143, 'Contradiction': 15, 'Understatement': 7}, ('Valid', 'y', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 142, 'Exaggeration': 18, 'General comment': 90, 'Cannot assess': 170, 'Contradiction': 15, 'Understatement': 9}, ('Valid', ']', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 368, 'Exaggeration': 82, 'General comment': 332, 'Cannot assess': 608, 'Contradiction': 56, 'Understatement': 12}, ('Valid', '[', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 306, 'Exaggeration': 98, 'General comment': 182, 'Cannot assess': 338, 'Contradiction': 30, 'Understatement': 12}, ('Valid', \"'\", 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 564, 'Exaggeration': 180, 'General comment': 320, 'Cannot assess': 606, 'Contradiction': 58, 'Understatement': 26}, ('Valid', 'n', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 202, 'Exaggeration': 72, 'General comment': 109, 'Cannot assess': 200, 'Contradiction': 21, 'Understatement': 10}, ('Valid', 'e', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 433, 'Exaggeration': 141, 'General comment': 248, 'Cannot assess': 435, 'Contradiction': 48, 'Understatement': 17}, ('Valid', 'u', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 119, 'Exaggeration': 26, 'General comment': 61, 'Cannot assess': 93, 'Contradiction': 4, 'Understatement': 6}, ('Valid', 'r', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 209, 'Exaggeration': 74, 'General comment': 123, 'Cannot assess': 222, 'Contradiction': 14, 'Understatement': 14}, ('Valid', 'o', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 192, 'Exaggeration': 61, 'General comment': 129, 'Cannot assess': 221, 'Contradiction': 22, 'Understatement': 15}, ('Valid', 'l', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 254, 'Exaggeration': 81, 'General comment': 149, 'Cannot assess': 309, 'Contradiction': 24, 'Understatement': 14}, ('Valid', 'g', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 63, 'Exaggeration': 21, 'General comment': 38, 'Cannot assess': 86, 'Contradiction': 10, 'Understatement': 9}, ('Valid', 'y', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 93, 'Exaggeration': 30, 'General comment': 48, 'Cannot assess': 89, 'Contradiction': 10, 'Understatement': 8}, ('Valid', ']', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 306, 'Exaggeration': 98, 'General comment': 182, 'Cannot assess': 338, 'Contradiction': 30, 'Understatement': 12}, ('Valid', '[', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 148, 'Exaggeration': 176, 'General comment': 250, 'Cannot assess': 508, 'Contradiction': 174, 'Understatement': 6}, ('Valid', \"'\", 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 276, 'Exaggeration': 286, 'General comment': 478, 'Cannot assess': 972, 'Contradiction': 346, 'Understatement': 6}, ('Valid', 'n', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 106, 'Exaggeration': 108, 'General comment': 164, 'Cannot assess': 354, 'Contradiction': 119, 'Understatement': 1}, ('Valid', 'e', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 194, 'Exaggeration': 228, 'General comment': 341, 'Cannot assess': 745, 'Contradiction': 240, 'Understatement': 5}, ('Valid', 'u', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 65, 'Exaggeration': 51, 'General comment': 91, 'Cannot assess': 184, 'Contradiction': 64, 'Understatement': 0}, ('Valid', 'r', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 101, 'Exaggeration': 107, 'General comment': 159, 'Cannot assess': 344, 'Contradiction': 155, 'Understatement': 0}, ('Valid', 'o', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 93, 'Exaggeration': 102, 'General comment': 171, 'Cannot assess': 386, 'Contradiction': 165, 'Understatement': 0}, ('Valid', 'l', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 107, 'Exaggeration': 136, 'General comment': 219, 'Cannot assess': 411, 'Contradiction': 160, 'Understatement': 4}, ('Valid', 'g', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 32, 'Exaggeration': 31, 'General comment': 50, 'Cannot assess': 148, 'Contradiction': 61, 'Understatement': 0}, ('Valid', 'y', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 44, 'Exaggeration': 46, 'General comment': 62, 'Cannot assess': 190, 'Contradiction': 60, 'Understatement': 1}, ('Valid', ']', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 148, 'Exaggeration': 176, 'General comment': 250, 'Cannot assess': 508, 'Contradiction': 174, 'Understatement': 6}, ('Valid', '[', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 262, 'Exaggeration': 174, 'General comment': 492, 'Cannot assess': 858, 'Contradiction': 100, 'Understatement': 6}, ('Valid', \"'\", 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 480, 'Exaggeration': 290, 'General comment': 932, 'Cannot assess': 1510, 'Contradiction': 186, 'Understatement': 12}, ('Valid', 'n', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 191, 'Exaggeration': 116, 'General comment': 335, 'Cannot assess': 543, 'Contradiction': 57, 'Understatement': 3}, ('Valid', 'e', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 343, 'Exaggeration': 233, 'General comment': 710, 'Cannot assess': 1046, 'Contradiction': 120, 'Understatement': 5}, ('Valid', 'u', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 98, 'Exaggeration': 47, 'General comment': 173, 'Cannot assess': 266, 'Contradiction': 28, 'Understatement': 2}, ('Valid', 'r', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 192, 'Exaggeration': 104, 'General comment': 367, 'Cannot assess': 574, 'Contradiction': 67, 'Understatement': 2}, ('Valid', 'o', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 148, 'Exaggeration': 79, 'General comment': 362, 'Cannot assess': 568, 'Contradiction': 86, 'Understatement': 3}, ('Valid', 'l', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 209, 'Exaggeration': 119, 'General comment': 410, 'Cannot assess': 713, 'Contradiction': 111, 'Understatement': 5}, ('Valid', 'g', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 60, 'Exaggeration': 18, 'General comment': 113, 'Cannot assess': 230, 'Contradiction': 40, 'Understatement': 1}, ('Valid', 'y', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 84, 'Exaggeration': 38, 'General comment': 135, 'Cannot assess': 257, 'Contradiction': 39, 'Understatement': 1}, ('Valid', ']', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 262, 'Exaggeration': 174, 'General comment': 492, 'Cannot assess': 858, 'Contradiction': 100, 'Understatement': 6}, ('Valid', 'c', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 175, 'Exaggeration': 40, 'General comment': 156, 'Cannot assess': 118, 'Contradiction': 45, 'Understatement': 0}, ('Valid', 'h', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 181, 'Exaggeration': 71, 'General comment': 194, 'Cannot assess': 121, 'Contradiction': 54, 'Understatement': 0}, ('Valid', 'i', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 223, 'Exaggeration': 53, 'General comment': 195, 'Cannot assess': 155, 'Contradiction': 61, 'Understatement': 0}, ('Valid', 'd', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 98, 'Exaggeration': 34, 'General comment': 110, 'Cannot assess': 87, 'Contradiction': 35, 'Understatement': 0}, ('Valid', ' ', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 206, 'Exaggeration': 54, 'General comment': 201, 'Cannot assess': 126, 'Contradiction': 54, 'Understatement': 0}, ('Valid', 'a', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 232, 'Exaggeration': 58, 'General comment': 221, 'Cannot assess': 137, 'Contradiction': 54, 'Understatement': 0}, ('Valid', 't', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 188, 'Exaggeration': 54, 'General comment': 196, 'Cannot assess': 115, 'Contradiction': 49, 'Understatement': 0}, ('Valid', 'c', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 320, 'Exaggeration': 45, 'General comment': 272, 'Cannot assess': 471, 'Contradiction': 41, 'Understatement': 22}, ('Valid', 'h', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 314, 'Exaggeration': 81, 'General comment': 303, 'Cannot assess': 518, 'Contradiction': 55, 'Understatement': 5}, ('Valid', 'i', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 389, 'Exaggeration': 60, 'General comment': 359, 'Cannot assess': 529, 'Contradiction': 53, 'Understatement': 6}, ('Valid', 'd', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 181, 'Exaggeration': 41, 'General comment': 184, 'Cannot assess': 294, 'Contradiction': 21, 'Understatement': 3}, ('Valid', ' ', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 375, 'Exaggeration': 59, 'General comment': 326, 'Cannot assess': 527, 'Contradiction': 51, 'Understatement': 12}, ('Valid', 'a', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 426, 'Exaggeration': 58, 'General comment': 347, 'Cannot assess': 631, 'Contradiction': 50, 'Understatement': 19}, ('Valid', 't', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 333, 'Exaggeration': 53, 'General comment': 319, 'Cannot assess': 504, 'Contradiction': 59, 'Understatement': 8}, ('Valid', 'c', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 239, 'Exaggeration': 72, 'General comment': 136, 'Cannot assess': 262, 'Contradiction': 29, 'Understatement': 15}, ('Valid', 'h', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 274, 'Exaggeration': 83, 'General comment': 156, 'Cannot assess': 334, 'Contradiction': 24, 'Understatement': 13}, ('Valid', 'i', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 318, 'Exaggeration': 81, 'General comment': 169, 'Cannot assess': 278, 'Contradiction': 26, 'Understatement': 9}, ('Valid', 'd', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 159, 'Exaggeration': 49, 'General comment': 93, 'Cannot assess': 166, 'Contradiction': 18, 'Understatement': 3}, ('Valid', ' ', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 297, 'Exaggeration': 84, 'General comment': 154, 'Cannot assess': 309, 'Contradiction': 25, 'Understatement': 15}, ('Valid', 'a', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 328, 'Exaggeration': 95, 'General comment': 170, 'Cannot assess': 338, 'Contradiction': 30, 'Understatement': 19}, ('Valid', 't', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 267, 'Exaggeration': 77, 'General comment': 159, 'Cannot assess': 278, 'Contradiction': 21, 'Understatement': 14}, ('Valid', 'c', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 121, 'Exaggeration': 129, 'General comment': 196, 'Cannot assess': 383, 'Contradiction': 139, 'Understatement': 2}, ('Valid', 'h', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 106, 'Exaggeration': 147, 'General comment': 241, 'Cannot assess': 408, 'Contradiction': 156, 'Understatement': 6}, ('Valid', 'i', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 162, 'Exaggeration': 162, 'General comment': 267, 'Cannot assess': 535, 'Contradiction': 168, 'Understatement': 4}, ('Valid', 'd', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 71, 'Exaggeration': 88, 'General comment': 135, 'Cannot assess': 292, 'Contradiction': 84, 'Understatement': 4}, ('Valid', ' ', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 137, 'Exaggeration': 143, 'General comment': 236, 'Cannot assess': 481, 'Contradiction': 172, 'Understatement': 3}, ('Valid', 'a', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 148, 'Exaggeration': 169, 'General comment': 253, 'Cannot assess': 508, 'Contradiction': 187, 'Understatement': 3}, ('Valid', 't', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 107, 'Exaggeration': 144, 'General comment': 218, 'Cannot assess': 401, 'Contradiction': 172, 'Understatement': 2}, ('Valid', 'c', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 213, 'Exaggeration': 133, 'General comment': 399, 'Cannot assess': 643, 'Contradiction': 63, 'Understatement': 5}, ('Valid', 'h', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 227, 'Exaggeration': 134, 'General comment': 429, 'Cannot assess': 733, 'Contradiction': 121, 'Understatement': 8}, ('Valid', 'i', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 264, 'Exaggeration': 162, 'General comment': 485, 'Cannot assess': 723, 'Contradiction': 76, 'Understatement': 8}, ('Valid', 'd', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 117, 'Exaggeration': 89, 'General comment': 251, 'Cannot assess': 360, 'Contradiction': 41, 'Understatement': 4}, ('Valid', ' ', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 253, 'Exaggeration': 139, 'General comment': 450, 'Cannot assess': 718, 'Contradiction': 97, 'Understatement': 6}, ('Valid', 'a', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 295, 'Exaggeration': 179, 'General comment': 511, 'Cannot assess': 837, 'Contradiction': 109, 'Understatement': 7}, ('Valid', 't', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 213, 'Exaggeration': 135, 'General comment': 443, 'Cannot assess': 665, 'Contradiction': 102, 'Understatement': 4}, ('Valid', ',', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 55, 'Exaggeration': 14, 'General comment': 52, 'Cannot assess': 33, 'Contradiction': 19, 'Understatement': 0}, ('Valid', 'm', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 14, 'Exaggeration': 8, 'General comment': 23, 'Cannot assess': 15, 'Contradiction': 9, 'Understatement': 0}, ('Valid', ',', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 106, 'Exaggeration': 14, 'General comment': 95, 'Cannot assess': 135, 'Contradiction': 16, 'Understatement': 5}, ('Valid', 'm', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 35, 'Exaggeration': 3, 'General comment': 48, 'Cannot assess': 59, 'Contradiction': 2, 'Understatement': 0}, ('Valid', ',', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 75, 'Exaggeration': 23, 'General comment': 39, 'Cannot assess': 88, 'Contradiction': 8, 'Understatement': 5}, ('Valid', 'm', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 25, 'Exaggeration': 10, 'General comment': 15, 'Cannot assess': 33, 'Contradiction': 3, 'Understatement': 1}, ('Valid', ',', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 41, 'Exaggeration': 34, 'General comment': 66, 'Cannot assess': 143, 'Contradiction': 50, 'Understatement': 0}, ('Valid', 'm', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 7, 'Exaggeration': 17, 'General comment': 26, 'Cannot assess': 33, 'Contradiction': 15, 'Understatement': 0}, ('Valid', ',', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 66, 'Exaggeration': 28, 'General comment': 121, 'Cannot assess': 202, 'Contradiction': 28, 'Understatement': 1}, ('Valid', 'm', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 22, 'Exaggeration': 25, 'General comment': 44, 'Cannot assess': 66, 'Contradiction': 13, 'Understatement': 1}, ('Valid', 's', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 166, 'Exaggeration': 28, 'General comment': 144, 'Cannot assess': 108, 'Contradiction': 38, 'Understatement': 0}, ('Valid', 'p', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 21, 'Exaggeration': 5, 'General comment': 28, 'Cannot assess': 7, 'Contradiction': 5, 'Understatement': 0}, ('Valid', 's', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 334, 'Exaggeration': 31, 'General comment': 225, 'Cannot assess': 400, 'Contradiction': 28, 'Understatement': 11}, ('Valid', 'p', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 54, 'Exaggeration': 4, 'General comment': 36, 'Cannot assess': 62, 'Contradiction': 4, 'Understatement': 1}, ('Valid', 's', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 240, 'Exaggeration': 67, 'General comment': 109, 'Cannot assess': 222, 'Contradiction': 22, 'Understatement': 7}, ('Valid', 'p', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 34, 'Exaggeration': 11, 'General comment': 17, 'Cannot assess': 32, 'Contradiction': 2, 'Understatement': 2}, ('Valid', 's', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 133, 'Exaggeration': 95, 'General comment': 184, 'Cannot assess': 418, 'Contradiction': 130, 'Understatement': 2}, ('Valid', 'p', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 15, 'Exaggeration': 13, 'General comment': 22, 'Cannot assess': 57, 'Contradiction': 18, 'Understatement': 0}, ('Valid', 's', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 227, 'Exaggeration': 111, 'General comment': 356, 'Cannot assess': 550, 'Contradiction': 46, 'Understatement': 6}, ('Valid', 'p', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 32, 'Exaggeration': 11, 'General comment': 54, 'Cannot assess': 68, 'Contradiction': 7, 'Understatement': 0}, ('Valid', 'w', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 16, 'Exaggeration': 1, 'General comment': 9, 'Cannot assess': 6, 'Contradiction': 2, 'Understatement': 0}, ('Valid', 'w', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 29, 'Exaggeration': 1, 'General comment': 10, 'Cannot assess': 26, 'Contradiction': 1, 'Understatement': 0}, ('Valid', 'w', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 21, 'Exaggeration': 3, 'General comment': 3, 'Cannot assess': 14, 'Contradiction': 0, 'Understatement': 1}, ('Valid', 'w', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 14, 'Exaggeration': 4, 'General comment': 12, 'Cannot assess': 29, 'Contradiction': 9, 'Understatement': 0}, ('Valid', 'w', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 27, 'Exaggeration': 6, 'General comment': 19, 'Cannot assess': 47, 'Contradiction': 5, 'Understatement': 1}, ('Valid', 'v', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 21, 'Exaggeration': 2, 'General comment': 19, 'Cannot assess': 13, 'Contradiction': 2, 'Understatement': 0}, ('Valid', 'v', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 26, 'Exaggeration': 6, 'General comment': 26, 'Cannot assess': 51, 'Contradiction': 5, 'Understatement': 0}, ('Valid', 'v', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 29, 'Exaggeration': 4, 'General comment': 15, 'Cannot assess': 26, 'Contradiction': 1, 'Understatement': 1}, ('Valid', 'v', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 16, 'Exaggeration': 8, 'General comment': 23, 'Cannot assess': 58, 'Contradiction': 12, 'Understatement': 0}, ('Valid', 'v', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 17, 'Exaggeration': 10, 'General comment': 40, 'Cannot assess': 46, 'Contradiction': 1, 'Understatement': 1}, ('Valid', 'f', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 24, 'Exaggeration': 1, 'General comment': 15, 'Cannot assess': 13, 'Contradiction': 6, 'Understatement': 0}, ('Valid', 'C', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 1, 'General comment': 6, 'Cannot assess': 9, 'Contradiction': 2, 'Understatement': 0}, ('Valid', '-', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 1, 'General comment': 6, 'Cannot assess': 9, 'Contradiction': 2, 'Understatement': 0}, ('Valid', '1', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 1, 'General comment': 6, 'Cannot assess': 9, 'Contradiction': 2, 'Understatement': 0}, ('Valid', '9', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 1, 'General comment': 6, 'Cannot assess': 9, 'Contradiction': 2, 'Understatement': 0}, ('Valid', 'f', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 40, 'Exaggeration': 3, 'General comment': 30, 'Cannot assess': 33, 'Contradiction': 5, 'Understatement': 0}, ('Valid', 'C', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 8, 'Exaggeration': 3, 'General comment': 12, 'Cannot assess': 25, 'Contradiction': 3, 'Understatement': 0}, ('Valid', '-', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 8, 'Exaggeration': 3, 'General comment': 12, 'Cannot assess': 25, 'Contradiction': 3, 'Understatement': 0}, ('Valid', '1', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 8, 'Exaggeration': 3, 'General comment': 12, 'Cannot assess': 25, 'Contradiction': 3, 'Understatement': 0}, ('Valid', '9', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 8, 'Exaggeration': 3, 'General comment': 12, 'Cannot assess': 25, 'Contradiction': 3, 'Understatement': 0}, ('Valid', 'f', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 30, 'Exaggeration': 5, 'General comment': 12, 'Cannot assess': 20, 'Contradiction': 3, 'Understatement': 0}, ('Valid', 'C', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 13, 'Exaggeration': 1, 'General comment': 7, 'Cannot assess': 11, 'Contradiction': 1, 'Understatement': 0}, ('Valid', '-', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 13, 'Exaggeration': 1, 'General comment': 7, 'Cannot assess': 11, 'Contradiction': 1, 'Understatement': 0}, ('Valid', '1', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 13, 'Exaggeration': 1, 'General comment': 7, 'Cannot assess': 11, 'Contradiction': 1, 'Understatement': 0}, ('Valid', '9', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 13, 'Exaggeration': 1, 'General comment': 7, 'Cannot assess': 11, 'Contradiction': 1, 'Understatement': 0}, ('Valid', 'f', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 23, 'Exaggeration': 6, 'General comment': 31, 'Cannot assess': 54, 'Contradiction': 11, 'Understatement': 0}, ('Valid', 'C', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 3, 'General comment': 16, 'Cannot assess': 31, 'Contradiction': 7, 'Understatement': 0}, ('Valid', '-', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 3, 'General comment': 16, 'Cannot assess': 31, 'Contradiction': 7, 'Understatement': 0}, ('Valid', '1', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 3, 'General comment': 16, 'Cannot assess': 31, 'Contradiction': 7, 'Understatement': 0}, ('Valid', '9', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 3, 'General comment': 16, 'Cannot assess': 31, 'Contradiction': 7, 'Understatement': 0}, ('Valid', 'f', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 28, 'Exaggeration': 8, 'General comment': 43, 'Cannot assess': 63, 'Contradiction': 1, 'Understatement': 1}, ('Valid', 'C', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 7, 'Exaggeration': 5, 'General comment': 22, 'Cannot assess': 26, 'Contradiction': 1, 'Understatement': 1}, ('Valid', '-', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 7, 'Exaggeration': 5, 'General comment': 22, 'Cannot assess': 26, 'Contradiction': 1, 'Understatement': 1}, ('Valid', '1', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 7, 'Exaggeration': 5, 'General comment': 22, 'Cannot assess': 26, 'Contradiction': 1, 'Understatement': 1}, ('Valid', '9', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 7, 'Exaggeration': 5, 'General comment': 22, 'Cannot assess': 26, 'Contradiction': 1, 'Understatement': 1}, ('Valid', 'k', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 17, 'Exaggeration': 6, 'General comment': 16, 'Cannot assess': 16, 'Contradiction': 3, 'Understatement': 0}, ('Valid', 'k', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 29, 'Exaggeration': 7, 'General comment': 28, 'Cannot assess': 43, 'Contradiction': 8, 'Understatement': 1}, ('Valid', 'k', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 27, 'Exaggeration': 9, 'General comment': 19, 'Cannot assess': 23, 'Contradiction': 1, 'Understatement': 0}, ('Valid', 'k', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 5, 'Exaggeration': 19, 'General comment': 16, 'Cannot assess': 46, 'Contradiction': 23, 'Understatement': 1}, ('Valid', 'k', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 15, 'Exaggeration': 18, 'General comment': 41, 'Cannot assess': 56, 'Contradiction': 15, 'Understatement': 0}, ('Valid', 'x', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 1, 'General comment': 12, 'Cannot assess': 4, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'x', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 15, 'Exaggeration': 3, 'General comment': 14, 'Cannot assess': 25, 'Contradiction': 2, 'Understatement': 0}, ('Valid', 'x', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 16, 'Exaggeration': 2, 'General comment': 8, 'Cannot assess': 11, 'Contradiction': 0, 'Understatement': 1}, ('Valid', 'x', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 6, 'Exaggeration': 5, 'General comment': 7, 'Cannot assess': 19, 'Contradiction': 5, 'Understatement': 0}, ('Valid', 'x', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 10, 'Exaggeration': 5, 'General comment': 16, 'Cannot assess': 12, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'b', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 8, 'Exaggeration': 4, 'General comment': 17, 'Cannot assess': 17, 'Contradiction': 10, 'Understatement': 0}, ('Valid', 'b', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 29, 'Exaggeration': 0, 'General comment': 25, 'Cannot assess': 33, 'Contradiction': 0, 'Understatement': 4}, ('Valid', 'b', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 18, 'Exaggeration': 9, 'General comment': 12, 'Cannot assess': 13, 'Contradiction': 6, 'Understatement': 1}, ('Valid', 'b', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 3, 'Exaggeration': 13, 'General comment': 9, 'Cannot assess': 15, 'Contradiction': 13, 'Understatement': 0}, ('Valid', 'b', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 9, 'Exaggeration': 17, 'General comment': 34, 'Cannot assess': 27, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'H', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 1, 'Exaggeration': 0, 'General comment': 1, 'Cannot assess': 1, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'I', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 1, 'Exaggeration': 0, 'General comment': 1, 'Cannot assess': 1, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'V', 'ChatGPT_prompt0_answer'): {'Agree with the gold answer': 1, 'Exaggeration': 0, 'General comment': 1, 'Cannot assess': 1, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'H', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 1, 'Exaggeration': 0, 'General comment': 2, 'Cannot assess': 2, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'I', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 1, 'Exaggeration': 0, 'General comment': 2, 'Cannot assess': 2, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'V', 'bingchat_prompt0_answer'): {'Agree with the gold answer': 1, 'Exaggeration': 0, 'General comment': 2, 'Cannot assess': 2, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'H', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 1, 'Cannot assess': 1, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'I', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 1, 'Cannot assess': 1, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'V', 'PerplexityAI_prompt0_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 1, 'Cannot assess': 1, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'H', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 0, 'Cannot assess': 2, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'I', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 0, 'Cannot assess': 2, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'V', 'llama2_70b_chat_prompt0_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 0, 'Cannot assess': 2, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'H', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 2, 'Cannot assess': 4, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'I', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 2, 'Cannot assess': 4, 'Contradiction': 0, 'Understatement': 0}, ('Valid', 'V', 'llama2_70b_chat_prompt1_answer'): {'Agree with the gold answer': 2, 'Exaggeration': 0, 'General comment': 2, 'Cannot assess': 4, 'Contradiction': 0, 'Understatement': 0}}\n"
     ]
    }
   ],
   "source": [
    "def create_combination_label_mapping(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a dictionary mapping every combination of question type, topic, and llm model name\n",
    "    to a dictionary of labels and their total occurrences per category.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        dict[[tuple], dict[str: int]]: A dictionary mapping every combination of question type, topic, and llm model name\n",
    "              to a dictionary of labels(6 categories) and their total occurrences.\n",
    "    \"\"\"\n",
    "    combination_label_mapping = {}\n",
    "    for qa_pair in qa_pairs:\n",
    "        for topic in qa_pair[3]:\n",
    "            combination = (qa_pair[0], topic, qa_pair[4])\n",
    "            if combination not in combination_label_mapping:\n",
    "                combination_label_mapping[combination] = {'Agree with the gold answer': 0, 'Exaggeration': 0, 'General comment': 0, 'Cannot assess': 0, 'Contradiction': 0, 'Understatement': 0}\n",
    "            for label in qa_pair[7]:\n",
    "                combination_label_mapping[combination][label] += 1\n",
    "        \n",
    "    \n",
    "    return combination_label_mapping\n",
    "print (create_combination_label_mapping(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc2407b4-c50d-4ee1-a095-6841e054e144",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7c4fbd69cb8719790f87966aadba9c4",
     "grade": true,
     "grade_id": "cell-d22ab5eabf38d10c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('2. Open ended - Comparison of different specific interventions', 'neurology', 'ChatGPT_prompt0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m)  \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m label_combinations\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m label_combinations\u001b[38;5;241m.\u001b[39mvalues()) \n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mlabel_combinations\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2. Open ended - Comparison of different specific interventions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneurology\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mChatGPT_prompt0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgree with the gold answer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExaggeration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneral comment\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot assess\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContradiction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnderstatement\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "\u001b[0;31mKeyError\u001b[0m: ('2. Open ended - Comparison of different specific interventions', 'neurology', 'ChatGPT_prompt0')"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "label_combinations = create_combination_label_mapping(data)\n",
    "assert isinstance(label_combinations, dict)\n",
    "assert all(isinstance(value, dict)  for value in label_combinations.values())\n",
    "assert all(len(value) == 6 for value in label_combinations.values()) \n",
    "assert label_combinations[('2. Open ended - Comparison of different specific interventions', 'neurology', 'ChatGPT_prompt0')] == {'Agree with the gold answer': 7, 'Exaggeration': 2, 'General comment': 3, 'Cannot assess': 4, 'Contradiction': 1, 'Understatement': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70394ff7-6b6c-4c7f-930b-a29cce6a6423",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9269c5f9c8feca0029e0b2f75ed3de1b",
     "grade": true,
     "grade_id": "cell-ae5ff64db01e2bf7",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ce4ef-2cd8-422d-8949-122b8052a635",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1ebca98044fa9f23967613df2724e22",
     "grade": false,
     "grade_id": "cell-cc8ef28a533658cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __e)__ (20 Points) \n",
    "In this task, we want to find the combinations with the highest occurrence for each label category(6). Create a dictionary that maps each label category to a list of combinations like in 1d) and its occurences for this label category but only include entries that have the highest occurence for a label category . If multiple combinations appear the same amount of times for this label category, all of them should be represented in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccbb5d-f5c5-4e46-8401-4df9e52f41bf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4672212308bb41a36e8efd58d60815e0",
     "grade": false,
     "grade_id": "cell-e03e7709c1018920",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_highest_combinations(combination_label_mapping: dict[Tuple[str, str, str], dict[str, int]]) -> dict:\n",
    "    \"\"\"\n",
    "    Finds combinations with the highest occurrence for each label.\n",
    "\n",
    "    Args:\n",
    "        combination_label_mapping (Dict): A dictionary mapping combinations to labels and their occurrences.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, List[Tuple[Tuple[str, str, str], int]]]: A dictionary mapping each label to a list of combinations with the highest occurrence of this label.\n",
    "                                                           Each entry in the list is a tuple containing the combination and its occurrence.\n",
    "    \"\"\"\n",
    "    result_mapping = {}\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return result_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8a125-e34d-4afc-9977-63b4379d21f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8039f2a69c9fc348ab4aecced543d719",
     "grade": true,
     "grade_id": "cell-4924c5d4eb1953cc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "highest_combinations = find_highest_combinations(label_combinations)\n",
    "assert isinstance(highest_combinations, dict)\n",
    "assert all(isinstance(value, list)  for value in highest_combinations.values())\n",
    "assert all(len(value[0]) == 2 for value in highest_combinations.values()) \n",
    "assert all(len(value[0][0]) == 3 for value in highest_combinations.values()) \n",
    "assert all(isinstance(value[0][1], int) for value in highest_combinations.values())\n",
    "assert len(highest_combinations.items()) == 6\n",
    "assert highest_combinations['General comment'] == [(('4. Open ended - General effects of a specific intervention', 'child health', 'llama2_70b_chat_prompt1'), 35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12724e7f-27be-426b-a905-32516909b6d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a60ac0dd0a5536c71065194c2742b3d3",
     "grade": true,
     "grade_id": "cell-d9626a0ffe1a50b3",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f7c3c-2c0c-470a-92d8-5d9f93404e5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ea2e78f5e001f62942e9cba79e0727d",
     "grade": false,
     "grade_id": "cell-0ab0d585563c31ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2 - 30 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7088436-b701-45f5-bb49-7bd3772b470b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a24a73aa577fd460bb8ecf1d0830a853",
     "grade": false,
     "grade_id": "cell-77b0ef79eefd4df4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In task 2 we will build a Decision Tree classifier and a Neural Network classifier and evaluate both afterwards. The first classifying task will ask the classifier to predict whether a given LLM answer is harmful or not. In the second classifying task, the classifier has to predict the label (6 categories) of a specific LLM answer unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba0b3d-beae-47dc-b37b-40336fb29e71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59790af3d445a0d20e0f3471c3b80b6c",
     "grade": false,
     "grade_id": "cell-516e5e8cd8043dd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell to import all modules needed for Task 2\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "## download this modules in case you have not downloaded them yet\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f63a1-2a7b-48be-9ee1-d46883823bbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75a078fdc2e8cf26f172794a92e07bd2",
     "grade": false,
     "grade_id": "cell-428680f13d820af6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __a)__ (15 Points)\n",
    "To achieve the first classification task mentioned above, we need to create a training and test split for the classifier. The input for the classifier should contain for each answer the question, the gold answer, the generated answer by the LLM, and additional features chosen by you that will improve the model's performance. You may use the extracted QA-Pairs from 1a). The dataset should be shuffled with the random seed 42. After shuffling the data, the dataset should be split on question level, meaning 80% of the QA-Pairs should be in the training split and 20% of the QA-Pairs should be in the test split. Prepare the data in such a way that for each data point all the following features: `question`,  `gold answer`, `large language model name`, `llm answer`, `num_answer_units`(the number of answer units generated by the LLM for each answer), `div_of_tokens`(the ratio of unique tokens of the LLM answer compared to all tokens of the generated answer by the LLM), `avg_len_words`(the average number of characters of all words for the given LLM answer) are represented as well as one of the two label categories(`harmful` and `non_harmful`). All of the highlighted features should be included and the features `num_answer_units`,`div_of_tokens` and `avg_len_words` should be only used for this task. Use **word_tokenize()** from nltk to tokenize the text, lowercase the tokens and remove punctuation and stopwords using `stopwords` from `nltk.corpus`. This task will be tested by testing functions and by evaluating the classifier's performance on the created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440ecae-22de-41c2-8fdd-b84642d09de2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72b36d2118bde1b58fe54972f4de00cc",
     "grade": false,
     "grade_id": "cell-ae281cdd4e2d125f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_train_test_split_answer(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]:\n",
    "    '''\n",
    "    Create a feature set and perform a train-test split. The train split is used to train the classifiers on. The test split is used to evaluate the classifiers on.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]: \n",
    "        A tuple containing two lists:\n",
    "        - The first list is the train split, where each entry is a tuple with a feature dictionary and its label (\"harmful\" or \"non_harmful\").\n",
    "        - The second list is the test split, with the same format.\n",
    "        - the feature dictionary should look like this: {\"question\": _, \"gold_answer\": _, \"llm_answer\": _, \"llm_model_name\": _,\"num_answer_units\": _,\"div_of_tokens\": _, \"avg_len_words\": _}\n",
    "        Example: The features: num_answer_units, div_of_tokens and avg_len_words would be: {num_answer_units': 4, 'div_of_tokens': 0.7384615384615385, 'avg_len_words': 5.859813084112149} for the following LLM answer: \n",
    "                 \"Macrolides have been studied as a potential treatment for chronic asthma, and their effectiveness compared to placebo varies depending on the specific study and patient population.\n",
    "                 Some studies suggest that macrolides may help reduce asthma exacerbations and improve lung function in certain individuals with chronic asthma, while others show no significant difference compared to a placebo. \n",
    "                 The effectiveness of macrolides in asthma management may depend on factors such as the patient's asthma phenotype, the specific macrolide used, and the duration of treatment. \n",
    "                 Therefore, it's essential to consult with a healthcare professional to determine if macrolides are a suitable option for managing chronic asthma in a particular case.\" \n",
    "    '''\n",
    "    train_split, test_split = [],[]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return train_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb867e0d-bbe9-47b7-be62-8e9e0687c651",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3eb7e3ea71fcbb1123278d237fcc09dd",
     "grade": true,
     "grade_id": "cell-142f873748dae1e6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "from numpy import isclose\n",
    "train_split_answer, test_split_answer = create_train_test_split_answer(data)\n",
    "assert isinstance(train_split_answer, list)\n",
    "assert isinstance(test_split_answer, list)\n",
    "assert all(isinstance(element[0], dict) for element in train_split_answer) \n",
    "assert all(isinstance(element[1], str) for element in train_split_answer) \n",
    "assert all(isinstance(element[0], dict) for element in test_split_answer) \n",
    "assert all(isinstance(element[1], str) for element in test_split_answer) \n",
    "assert all(\"num_answer_units\" in element[0].keys() for element in train_split_answer)\n",
    "assert all(\"num_answer_units\" in element[0].keys() for element in test_split_answer)\n",
    "assert all(\"div_of_tokens\" in element[0].keys() for element in train_split_answer)\n",
    "assert all(\"div_of_tokens\" in element[0].keys() for element in test_split_answer)\n",
    "assert all(\"avg_len_words\" in element[0].keys() for element in train_split_answer)\n",
    "assert all(\"avg_len_words\" in element[0].keys() for element in test_split_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac79d2-7aa8-44be-a502-0cc46d65ef27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0038f11d4e903b71cbdcaa00f7f9eb4",
     "grade": true,
     "grade_id": "cell-11dd4292ab763d65",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698dcb2-e6a8-45c3-85de-52806a52c454",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf760a892d79d7c7baf1d00ba5a7a7df",
     "grade": false,
     "grade_id": "cell-07d25e5e6e5b3a82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __b)__ (15 Points)\n",
    "For the second classification task , we also need to create a training and test split for the classifier. The input for the classifier should contain for each answer the question, the gold answer, the generated answer units by the LLM, and additional features chosen by you that will improve the model's performance. You may use the extracted QA-Pairs from 1a). The dataset should be shuffled with the random seed 42. After shuffling the data, the dataset should be split on question level, meaning 80% of the QA-Pairs should be in the training split and 20% of the QA-Pairs should be in the test split. Prepare the data in such a way that for each data point all the following features: `question`,  `gold answer`, `large language model name`, `llm answer unit`, `trigrams`(create a list of all trigrams for the tokenized an preprocessed answer unit and convert it into one string for adding to the feature dictionary, you can use `ngrams` for creating the list), `word_count`(the number of words in the llm answer unit), `token_overlap`(the number of unique tokens that overlap between the gold answer and the LLM answer unit) are represented as well as one of the six label categories. All of the highlighted features should be included and the features `trigrams`,`word_count` and `token_overlap` should be only used for this task. Preprocess the data for `trigrams`,`word_count` and `token_overlap` in the same way as in 2a) before extracting the features and **exclude** empty answer units. This task will be tested by testing functions and evaluating the classifiers perfomance on the created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1b0d9-3931-4606-97b2-809b4b6ab356",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "695fcfb046dcf05efebbb2828c6a607f",
     "grade": false,
     "grade_id": "cell-1803ea5be23673a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_train_test_split_answer_units(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]:\n",
    "    '''\n",
    "    Create a feature set and perform a train-test split. The train split is used to train the classifiers on. The test split is used to evaluate the classifiers on.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]: \n",
    "        A tuple containing two lists:\n",
    "        - The first list is the train split, where each entry is a tuple with a feature dictionary and its label (6 categories).\n",
    "        - The second list is the test split, with the same format.\n",
    "        - the feature dictionary should look like this: {\"question\": _, \"gold_answer\": _, \"llm_model_name\": _, \"llm_answer_unit\": _, \"trigrams\": _, \"word_count\": _, \"token_overlap\": _}\n",
    "        Example: The features: trigrams, word_count and token_overlap would be: {'trigrams': 'intrathecal nusinersen shown nusinersen shown effective shown effective treating effective treating infants treating infants spinal infants spinal muscular spinal muscular atrophy muscular atrophy SMA atrophy SMA type SMA type compared type compared sham compared sham procedure',\n",
    "                 'word_count': 14, 'token_overlap': 7} for the following LLM answer unit and gold answer: \n",
    "                 'llm_answer_unit': 'Intrathecal nusinersen has been shown to be effective in treating infants with spinal muscular atrophy (SMA) type I, compared to sham procedure.'\n",
    "                 'gold_answer': 'Reviewers conducted a search in October 2018 and found only a single small RCT assessing nusinersen in infants with SMA type I. Nusinersen may reduce the combined outcome of death or need for full‐time ventilation, and infants may experience fewer severe adverse events compared with a sham procedure; effects on other outcomes were unclear.\n",
    "                                 No firm conclusions can be drawn based on this small study. In addition to the paucity of evidence, nusinersen is incredibly expensive (125,000 USD per dose, with at least four doses in the first year of treatment), is dosed via lumbar/spinal tap, and may require sedation and special treatment centers for administration. Click here for further information.'\n",
    "    '''\n",
    "    train_split, test_split = [],[]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return train_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc01957-0156-4bb5-997e-74e62f7ff017",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18193d3b4fa196839ee7ab130c11ff7b",
     "grade": true,
     "grade_id": "cell-51803d5fb1b87b36",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "train_split_answer_units, test_split_answer_units = create_train_test_split_answer_units(data)\n",
    "assert isinstance(train_split_answer_units, list)\n",
    "assert isinstance(test_split_answer_units, list)\n",
    "assert isinstance(train_split_answer_units[0][0], dict)\n",
    "assert isinstance(train_split_answer_units[0][1], str)\n",
    "assert isinstance(test_split_answer_units[0][0], dict)\n",
    "assert isinstance(test_split_answer_units[0][1], str)\n",
    "assert all(\"trigrams\" in element[0].keys() for element in train_split_answer_units)\n",
    "assert all(\"trigrams\" in element[0].keys() for element in test_split_answer_units)\n",
    "assert all(\"word_count\" in element[0].keys() for element in train_split_answer_units)\n",
    "assert all(\"word_count\" in element[0].keys() for element in test_split_answer_units)\n",
    "assert all(\"token_overlap\" in element[0].keys() for element in train_split_answer_units)\n",
    "assert all(\"token_overlap\" in element[0].keys() for element in test_split_answer_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aca43a-de04-408d-9033-4f6129d9d7be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aa5bc06ed1e816c8bb865a78b0822b9",
     "grade": true,
     "grade_id": "cell-8ad59196ba5a5cbf",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6cdc4-5f12-4740-8511-8158bf09016e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1454de60d071a5f842d800b2364d46c7",
     "grade": false,
     "grade_id": "cell-90572afeb33fefed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __c)__ (0 Points)\n",
    "This function is used for training the classifier on the training split created in 2a) and 2b). Feel free to use this function to test the training split created by you. You must use the same vectorizer object for training and testing the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708be5a-d01e-49f5-8d77-7701a67886f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dba6602c1bb3cdd00302985afb099a9",
     "grade": false,
     "grade_id": "cell-686c37efdb916e71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_cls(train_split: List[Tuple[dict, str]], vectorizer: DictVectorizer) -> Tuple[nltk.DecisionTreeClassifier, MLPClassifier]:\n",
    "    \"\"\"\n",
    "    Train a Decision Tree Classifier (DTC) and a Neural Network Classifier using the provided data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_split (List[Tuple[dict, str]]): A list of tuples containing feature dictionaries and their corresponding labels.\n",
    "    - vectorizer (DictVectorizer): An instance of DictVectorizer for converting feature dictionaries into a vector representation.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[nltk.DecisionTreeClassifier, MLPClassifier]: A tuple containing the trained Decision Tree Classifier (DTC) and Neural Network Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the train set into features and labels\n",
    "    feature_train, labels_train = zip(*train_split)\n",
    "    \n",
    "    # Train the Decision Tree Classifier\n",
    "    dtc = nltk.DecisionTreeClassifier.train(train_split)\n",
    "\n",
    "    # Embed the names into a vector representation\n",
    "    train_vectorized = vectorizer.fit_transform(feature_train)\n",
    "    \n",
    "    # Create the Neural Network Model and train it on the data\n",
    "    neural_network_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "    neural_network_classifier.fit(train_vectorized, labels_train)\n",
    "\n",
    "    return dtc, neural_network_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ac11e-9e2c-43c0-ac98-7cfb490390c4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c58e6b6daade16051da10aeab4b8f8a4",
     "grade": false,
     "grade_id": "cell-f54339d48dd1630c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to train the classifier on the training split you created.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97d1be-e843-45cc-b067-cfc7633a5e47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2d37c2958d8780425e1f00e18d1d570",
     "grade": true,
     "grade_id": "cell-93ea6202de1d7198",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124743e-3245-4e89-94fb-7ed09f67536d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bfd5a1530870c567712756e3e1fe3f3",
     "grade": false,
     "grade_id": "cell-9a5a2189104d9ec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __d)__ (0 Points)\n",
    "This function returns the precision, recall and f1 for a given classifier and label category. Use this function to test the test split created by you.\n",
    "You must use the same vectorizer object for training and testing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1e706-f41b-466e-836c-eb5236d4382f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "497682e5319564c4528271e33183f90b",
     "grade": false,
     "grade_id": "cell-c391bb3a35504bd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, vectorizer:DictVectorizer, test_split: List[Tuple[dict, str]], is_nn: bool, label_category: str) -> Tuple[float, float, float]:\n",
    "    '''\n",
    "    Evaluate a classifier using precision, recall, and F1 score for a specific label category.\n",
    "\n",
    "    Args:\n",
    "        classifier (object): The trained classifier model.\n",
    "        vectorizer (DictVectorizer): The vectorizer used to transform features.\n",
    "        test_split (List[Tuple[Dict[str, str], str]]): The test set is a list, where each entry is a tuple with a feature dictionary and its label.\n",
    "        is_nn (bool): Indicates if the given classifier is a neural network (MLPClassifier), otherwise, it is a nltk.DecisionTreeClassifier.\n",
    "        label_category (str): The specific label category for which the evaluation is performed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: A tuple containing precision, recall, and F1 score.\n",
    "    '''\n",
    "    precision, recall, f_score = 0, 0, 0\n",
    "    ### Begin Solution\n",
    "    feature_test, labels_test = zip(*test_split)\n",
    "    predictions = []\n",
    "    if is_nn:\n",
    "        # vetorize test features\n",
    "        test_vectorized = vectorizer.transform(feature_test)\n",
    "        predictions =  classifier.predict(test_vectorized)\n",
    "    else: \n",
    "        predictions = [classifier.classify(features) for features in feature_test]\n",
    "    precision = precision_score(labels_test, predictions, labels=[label_category], average='weighted',zero_division=0)\n",
    "    recall= recall_score(labels_test, predictions, labels=[label_category], average='weighted',zero_division=1)\n",
    "    f_score= f1_score(labels_test, predictions, labels=[label_category], average='weighted', zero_division=0)\n",
    "    ### End Solution\n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb7aac-693f-461c-a2f2-652917f66187",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03ca7e5a0f5321f6e412c8d4e56c9fe6",
     "grade": false,
     "grade_id": "cell-f49b1a4f8fdba927",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to evaluate the classifier on the test split you created. \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece9b05-4394-47f2-acd7-d98e9581feeb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ba86f14218e42680b2a772bcbd08680",
     "grade": true,
     "grade_id": "cell-af27df1fc855fb8f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
